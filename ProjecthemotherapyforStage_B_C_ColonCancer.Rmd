---
title: "eH705 Project, due 12-Apr-2021"
subtitle: "hemotherapy for Stage B/C colon cancer"
author:
  name: Neha Kalra, Matthew Jabora, Ibrahim Zifa, Sandeep Delar, Romuald Lamps 
  affiliation: eH705 | eHealth MsC
date: "12 Apr 2021"
output:
  html_document: 
    number_sections: TRUE
    code_folding: hide
    toc: yes
    toc_float: 
      toc_collapsed: true
    theme: readable
---  

```{r setup, eval=TRUE, include=FALSE}  
knitr::opts_chunk$set( echo=TRUE, warning=FALSE, message=FALSE, error=FALSE, comment = NA )
options(max.print = 10000)
options(scipen = 999)
pacman::p_load(DT, kableExtra, magrittr, ggplot2, wrapr, generics, car)
```   

```{r "colorize", echo=FALSE, results="hide"}
colorize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color, 
      x)
  } else x
}
```  
# Instructions  
Your analysis should follow the steps taken in this course:
1. Read the material provided. Sometimes very little is mentioned about the data. In all cases, I’ve provided everything I have about the dataset that you could use. You may find more information through online searches. (Good datasets are very difficult to obtain. )
2. Set up your R Markdown file and read the dataset. This may be done in RStudio or in R Commander. Feel free to use either R-code directly in RStudio (R Markdown) or in R Commander (producing an R Markdown document).
3. **Exploratory Data Analysis**. Produce basic tables and graphs so that you understand the data and can explain the basics in your report. You may have to google the literature to get background on some data and medical terms.
4. You may have to convert some variables to factors and will likely need to recode to provide value labels for factor variables.
5. Check on missing values and determine the best way of handling missing values.
6. There should be one variable that qualifies as a response or dependent variable. Your focus for much of your work will be to investigate how to explain and/or predict values of the response variable by using predictor variables.
7. Your work in point 6 will likely include some form of regression and, perhaps ANOVA and correlation. The regression may be linear regression or logistic regression. That will depend on the nature of the response variable. Sometimes the analysis in the journal article was conducted using survival analysis. While that topic is covered in this course, you may choose to use survival analysis or logistic regression.
8. Explaining your work and the findings is extremely important. Naturally, this is a statistics course and you may use statistical terminology and notation in your reports. However, you must show that you understand the material and can explain it in plain language to others who have limited statistical backgrounds. Simply producing a mass of statistical output with no or very little explanation will not attract a very high mark.

# Analysis used in the study  
Levamisole use in combination with flourouracil in the study is based on the hope that it would add to the marginal activity of flourouracil. The aim of the study is to confirm that adjuvant therapy with the combination of levamisole and fluorouracil significantly reduced the cancer-recurrence rate as compared with the rate when no adjuvant therapy was given, where as the use if levamisole alone produced a borderline advantage. Survival was the primary end point of the study, the time to recurrence was also determined.
1296 patients with resected colon cancer that either was locally invasive (Stage B2) or had regional nodal involvement (Stage C) were randomly assigned to observation or to treatment for one year with levamisole combined with flourocil.
Analysis conducted by the case study
1. Survival curves by the Kaplan- Merier Method [The Kaplan–Meier estimator also known as the product limit estimator, is a non-parametric statistic used to estimate the survival function from lifetime data. In medical research, it is often used to measure the fraction of patients living for a certain amount of time after treatment.]
2. The Log Rank Test: It was used to determine the ratios of relapse and survival rates and to perform all multivariate analysis. Long rank test description:
a. We are often interested in assessing whether there are differences in survival (or cumulative incidence of event) among different groups of participants. For example, in a clinical trial with a survival outcome, we might be interested in comparing survival between participants receiving a new drug as compared to a placebo (or standard therapy).
b. The log rank test is a popular test to test the null hypothesis of no difference in survival between two or more independent groups.
c. a test of whether the survival curves are identical (overlapping) or not. 
d. Survival curves are estimated for each group, considered separately, using the Kaplan-Meier method and compared statistically using the log rank test.
• The log rank test is a non-parametric test and makes no assumptions about the survival distributions. In essence, the log rank test compares the observed number of events in each group to what would be expected if the null hypothesis were true (i.e., if the survival curves were identical).
• H0: The two survival curves are identical (or S1t = S2t) versus H1: The two survival curves are not identical (or S1t ≠ S2t, at any time t) (α=0.05).
• The log rank statistic is approximately distributed as a chi-square test statistic. There are several forms of the test statistic, and they vary in terms of how they are computed.
3. Cox proportional-hazards model was used to determine the ratios of relapse and survival rates.
• The Cox proportional-hazards model (Cox, 1972) is essentially a regression model commonly used statistical in medical research for investigating the association between the survival time of patients and one or more predictor variables.
• Kaplan-Meier curves and logrank tests - are examples of univariate analysis. They describe the survival according to one factor under investigation, but ignore the impact of any others.
• Additionally, Kaplan-Meier curves and logrank tests are useful only when the predictor variable is categorical (e.g.: treatment A vs treatment B; males vs females). They don’t work easily for quantitative predictors such as gene expression, weight, or age.
• Cox proportional hazards regression analysis, which works for both quantitative predictor variables and for categorical variables. Furthermore, the Cox regression model extends survival analysis methods to assess simultaneously the effect of several risk factors on survival time.
4. Backward regression was used to find the significant prognostic factors, variables were progressively eliminated on the basis of the maximal partial-liklihood estimate (MLE) statistics.
• All P values reported for this study are two sided.
• Stepwise regression is the step-by-step iterative construction of a regression model that involves the selection of independent variables to be used in a final model. It involves adding or removing potential explanatory variables in succession and testing for statistical significance after each iteration.
• The underlying goal of stepwise regression is, through a series of tests (e.g. F-tests, t-tests) to find a set of independent variables that significantly influence the dependent variable. 
• Stepwise regression can be achieved either by trying out one independent variable at a time and including it in the regression model if it is statistically significant or by including all potential independent variables in the model and eliminating those that are not statistically significant.
• An example of a stepwise regression using the backward elimination method would be an attempt to understand energy usage at a factory using variables such as equipment run time, equipment age, staff size, temperatures outside, and time of year. The model includes all of the variables—then each is removed, one at a time, to determine which is least statistically significant. In the end, the model might show that time of year and temperatures are most significant, possibly suggesting the peak energy consumption at the factory is when air conditioner usage is at its highest. 
5. Exploratory subset Analysis
Levamisole-fluorouracil treatment appeared to have the greatest advantage among
• the male patients (in both survival and recurrence),
• older patients (recurrence)
• Patients with tumours that were well differentiated to moderately well differentiated (survival and recurrence)
• patients in whom more than four nodes were involved(survival)
• patients treated 21 to 35 days after surgery (recurrence)
• These results show striking contradictions to those of subset analyses in which levamisole plus fluorouracil was found to be most effective in reducing the risk of recurrence among female patients and younger patients.

# Objectives  
We wish to confirm using our own process that adjuvant therapy with the combination of levamisole and fluorouracil significantly reduce death within 5 years compared with no adjuvant therapy – using only levamisole.  We chose to only include these two groups because we are only comparing adjuvant therapy against normal therapy.  Levimasole is already recognized worldwide as an antiparasitic drug that changes ones immune response, similar to methotrexate – so we are considering it a control.  We aim to achieve this by:
1.  To assess the distributions of variables within the two treatment categories, including: (distribution plots/tables between groups)
    a.	Sex – Male or female patients
    b.	Age – Finding the average (or median) age that has most impact on 5 year survival
    c.	Obstructive tumours versus non-obstructive
    d.	If the colon was perforated by the tumor or not
    e.	If tumours were adhered to nearby organs
    f.	Differentiation of tumors – rated well, moderate, or poor
    g.	The extent of local spread of the tumor which could be:
        i.	Submucosal
        ii.	Within muscle
        iii.	Within the serosa – the outermost layer of organs
        iv.	Contiguous structures – meaning nearby organs
2.  Investigate if any of the predictor variables have an impact on each other. (correlation)(VIF)
3.  Using stepwise regression to obtain a model that best describes the impact of specific variables on five year life status
4.	Using a logistic regression model to investigate the association between five year life status and one or more categorical predictor variables within each treatment group (regression models for both groups) (predict) (train+test) (residuals)
5.	Assessing whether there are differences in survival (or cumulative incidence of event) among different groups of participants
    a.  This will be done by testing the null hypothesis of no difference in survival between two or more independent groups. 
        •	H0: The two survival rates are identical, (α=0.05).    
        •	Ha: The two survival rates are not identical.  (chi-square test)
6.	Analyze data within the subset of Levamisole-fluorouracil to find which variables have the greatest impact on 5 year mortality.  (Reliampo)

# Read the file  
```{r}
colon.data <- read.csv('colon_s.csv')
```

# Cleaning and Recoding the Data  
## Column descriptions  
```{r}
df = as.data.frame(matrix( 
  c('id','id',
    'rx','Treatment - Obs(ervation), Lev(amisole), Lev(amisole)+5-FU',
    'sex','sex of the patient (1=Male)',
    'age','age in years',
    'obstruct','obstruction of colon by tumour',
    'perfor','perforation of colon',
    'adhere','adherence to nearby organs',
    'nodes','number of lymph nodes with detectable cancer',
    'status','censoring status',
    'differ','differentiation of tumour (1=well, 2=moderate, 3=poor)',
    'extent','Extent of local spread (1=submucosa, 2=muscle, 3=serosa,
4=contiguous structures)',
    'surg','time from surgery to registration (0=short, 1=long)',
    'node4','more than 4 positive lymph nodes',
    'time','days until death',
    'loccomp','????',
    'time.years','years until death',
    'mort_5yr','5 years life status',
    'age.10','Age divided by 10..',
    'hospital','Hospital'), # the data elements 
  ncol = 2,              # number of rows 
  byrow = TRUE))
colnames(df) <- c("Variable", "Description")

df %>%
  kable(caption = "World Population Health: All Countries") %>%
  kable_styling( bootstrap_options = "striped", full_width = F, position = "left", fixed_thead = T) 
```

## Converting Variables  
```{r}
library(dplyr)
colon.data <- colon.data %>% mutate_if(is.character, as.factor)
colon.data <- colon.data %>% mutate_if(is.integer, as.numeric)
colon.data <- colon.data %>%
  select(rx, sex, age, obstruct, perfor, adhere, differ, extent, mort_5yr, rx.factor, sex.factor, age.factor, obstruct.factor, perfor.factor, adhere.factor, differ.factor, extent.factor, mort_5yr.num) %>%
  filter(rx.factor!="Obs") 
library(Hmisc)
describe(colon.data)
```

## Missing Values and Treatment  
In this section, the total count of missing values in each column is identified. Since the total count of NA is less than 5%, we will not treat the missing values. *Note - Zeros are not treated as part of treating missing values as zeros are one of the valid response outputs.*
```{r}
MD <- colSums(is.na(colon.data))
library(kableExtra)
MD %>%
  kable(caption = "Table 3: Total count of NA for each variable") %>%
   kable_styling( bootstrap_options = "striped", full_width = F, position = "left", fixed_thead = T) 
```

# Cleaned Data  
## The Data Frame  
```{r}
colon.data %>% datatable()
```

## Observations for the data  
```{r}
densityPlot(~age, data = colon.data, bw = bw.SJ, adjust = 1, kernel = dnorm, method = "adaptive")
densityPlot(~time.years, data = colon.data, bw = bw.SJ, adjust = 1, kernel = dnorm, method = "adaptive")
bar_charting <- function(gplt, nm)(gplt + scale_fill_brewer(palette="Blues", name = nm) + geom_bar(aes(y =(..count..)/sum(..count..))) + theme_gray() + scale_y_continuous(labels=scales::percent) + ylab("") + xlab(nm) + guides(x =  guide_axis(angle = 45)))
bar_charting(ggplot(colon.data, aes(x=rx.factor, y="", fill=rx.factor)), "Treatment")
bar_charting(ggplot(colon.data, aes(x=nodes, y="", fill=nodes)), "number of lymph nodes with detectable cancer")
bar_charting(ggplot(colon.data, aes(x=differ.factor, y="", fill=differ.factor)), "differentiation of tumour")
bar_charting(ggplot(colon.data, aes(x=extent.factor, y="", fill=extent.factor)), "Extent of local spread")
bar_charting(ggplot(colon.data, aes(x=surg.factor, y="", fill=surg.factor)), "time from sugery to registration")
```

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

Few observations looking at the data in graphs above:

* Below are the columns identified with NA:
   * Obstruct - 21
   * nodes - 18
   * differ - 23
   * surg - 17
   * loccomp -20
   * mort_5yr - 14
* Since the total count of NA is less than 5%, we will not treat the missing values.
</div>

*** 

# Exploring the distribution of the data  
Below tree diagram shows:

* Total number of observations- 929
* 33% of the observations are for patients who were given levamisole (Lev).
* 33% were given levamisole + fluorouracil (Lev +5FU)
* 34% constitutes the controls group.
* there is almost equal distribution of male and females in both the groups (Lev and Lev+5FU)
* Majority of the patients belong to the age group of 60+ followed by 40-59 and <40 years.

```{r echo=FALSE, , fig.align="center", fig.width=6, fig.height=6, fig.cap="Figure 2:add"}
library(vtree)
vtree(colon.data, c("rx.factor", "sex.factor", "age.factor"), horiz = FALSE, fillcolor = c(restecg = "#e7d4e8", 
  cp = "#99d8c9"), keep = list(restecg = c("ST-T abnormality")))
```

**Frequencies and percentages by Sex**  
Below table shows the distribution of sexes in the different treatment therapies
```{r}
library(sjPlot)
x<- sjt.xtab(colon.data$rx.factor, colon.data$sex.factor, show.col.prc = TRUE,
             CSS = list(css.table = "border: 2px solid;",
                    css.tdata = "border: 1px solid;"))
x
```

<br>

**Frequencies and percentages by Age Group**  
```{r}
y<- sjt.xtab(colon.data$rx.factor, colon.data$age.factor, show.col.prc = TRUE,
             CSS = list(css.table = "border: 2px solid;",
                    css.tdata = "border: 1px solid;"))
y
```
<br>

**Frequencies and percentages by obstruction**  
```{r}
z<- sjt.xtab(colon.data$rx.factor, colon.data$obstruct.factor, show.col.prc = TRUE,
             CSS = list(css.table = "border: 2px solid;",
                    css.tdata = "border: 1px solid;"))
z
```
<br>

**Frequencies and percentages by Perforance**  
```{r}
per<- sjt.xtab(colon.data$rx.factor, colon.data$perfor.factor, show.col.prc = TRUE,
             CSS = list(css.table = "border: 2px solid;",
                    css.tdata = "border: 1px solid;"))
per
```
obstruct.factor
</div>


# Logistic regression using mort_5yr  
We decided to use mort_5yr as the response variable.

## Split the data into train and test set  
```{r}
library(rsample)
set.seed(78)
train_test_split <- initial_split(colon.data, prop=0.7)
train <- training(train_test_split)
test <- testing(train_test_split)
(samp <- dim(train_test_split))
```

## NULL model  
**TODO** interpretation of the intercept

```{r}
null_model <- glm(mort_5yr ~ 1, family= binomial(logit), data = train)
summary(null_model)
```

## Use all the variables to perform a logistic regression   
The treatment seems to be the only predictor of the 5 year life status. 

```{r}
model_training <- glm(mort_5yr ~ rx + sex + age + obstruct + perfor + adhere + differ + extent, family= binomial(logit), data = train)
summary(model_training)
```

## Use the extent of tumor as predictor  
**TODO** Interpretation of the result

```{r}
model_training_rx_only <- glm(mort_5yr ~ rx, family= binomial(logit), data = train)
summary(model_training_rx_only)
```

## Analysis to see if the treatment make a difference in   
**TODO** explain p nd chi-square values

```{r}
library(sjPlot)
tab_xtab(colon.data$mort_5yr, colon.data$rx , show.col.prc=TRUE, show.row.prc=TRUE, var.labels=c("Life status After 5 year", "Treatment"))
```

## Multicollinearity. 
No multicollinearity is required here, as only one predictor was required.

## Anova.  
Here I build the NULL model using the training data set, and I use ANOVA to check if the 2 models deviance are significantly different.

The ANOVA test below indicates that the combined Lev+5FU treatment has a significant impact on the 5 year life status at the 1% level of risk.

```{r}
anova(model_training_rx_only, null_model, test ="Chisq")
```

## Predict against test data. 
```{r}
test_results <- predict(model_training_rx_only, newdata=test)
```

## Hit Ratio. 
**TODO** Not working and I couldn't figure out why...
```{r}
predicted.prob <- predict( model_training_rx_only, newdata=test, type = "response")

str(predicted.prob)
```

```{r}
library(forcats)
predicted.classes <- as.factor( ifelse( predicted.prob > 0.5, "Alive", "Death" ) )
predicted.classes <- fct_relevel( (predicted.classes) , c("Alive","Death" ) )
# tab_xtab(test$mort_5yr, predicted.classes, var.labels = c("Actual Life status", "Predicted life status"), show.row.prc = TRUE)
```

### how are the residuals. 
## Conclusion on the logistic regression test. 


# Summative Conclusion. 

**TODO_test3**

**TEST_sdharma108**

---  
title: 'Regression Analysis mort_5yr.num: 5 years life status'
author:
  name:
  affiliation: eH705 | eHealth MSc Program, McMaster University
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    color: red
    number_sections: yes
    code_folding: hide
    toc: yes
    toc_float:
      toc_collapsed: yes
    theme: readable
    font_family: Tahoma, Tahoma, seriff
  pdf_document:
    toc: yes
---
```{r echo=FALSE, message=FALSE}
# include this code chunk as-is to set options
knitr::opts_chunk$set(comment=NA, prompt=TRUE, echo = TRUE, message = FALSE, warning = FALSE, error = FALSE)
library(Rcmdr)
library(car)
library(RcmdrMisc)
```
```{r  "setup", include=FALSE}  
pacman::p_load(psych,xray , ggplot2, texreg, dplyr,wrapr,  DT,  sjPlot, knitr, DescTools, Hmisc, corrplot,ggcorrplot, qgraph, corrr, tidyverse,sjmisc, sjlabelled, sjstats, sjPlot,  forcats,kableExtra, captioner, unikn,   kableExtra, captioner, car,MASS, hrbrthemes, relaimpo,plotflow, tidyverse, dials, tidymodels, ggeffects, haven,glmmTMB, broom, magick,BiocManager  )

```
```{r echo=FALSE}
# include this code chunk as-is to enable 3D graphs
library(rgl)
knitr::knit_hooks$set(webgl = hook_webgl)
```
```{r echo=FALSE}
install.packages('BiocManager')
```

# Reading the Data 
```{r echo=TRUE, message=FALSE, warning=FALSE, fig.width=5, fig.height=5, comment=NA}
colon.data <- 
  read.csv("C:/Users/Sandeep Delar/Desktop/EHEALTH 705Statistics for eHealth -ASSIGNMENT/Group Proect 705/colon_s.csv",
   header=TRUE, stringsAsFactors=TRUE, sep=",", na.strings="NA", dec=".", 
  strip.white=TRUE)
```
```{r}
df = as.data.frame(matrix( 
  c('id','id',
    'rx','Treatment - Obs(ervation), Lev(amisole), Lev(amisole)+5-FU',
    'sex','sex of the patient (1=Male)',
    'age','age in years',
    'obstruct','obstruction of colon by tumour',
    'perfor','perforation of colon',
    'adhere','adherence to nearby organs',
    'nodes','number of lymph nodes with detectable cancer',
    'status','censoring status',
    'differ','differentiation of tumour (1=well, 2=moderate, 3=poor)',
    'extent','Extent of local spread (1=submucosa, 2=muscle, 3=serosa,
4=contiguous structures)',
    'surg','time from surgery to registration (0=short, 1=long)',
    'node4','more than 4 positive lymph nodes',
    'time','days until death',
    'loccomp','????',
    'time.years','years until death',
    'mort_5yr','5 years life status',
    'age.10','Age divided by 10..',
    'hospital','Hospital'), # the data elements 
  ncol = 2,              # number of rows 
  byrow = TRUE))
colnames(df) <- c("Variable", "Description")

df %>%
  kable(caption = "World Population Health: All Countries") %>%
  kable_styling( bootstrap_options = "striped", full_width = F, position = "left", fixed_thead = T) 
```




# Removing Missing Values 

```{r}
 colon.data= na.omit(colon.data)
```

******

# A regression of ‘mort_5yr’ on sex, age, , obstruct, , perfor,adhere ,nodes , status, differ, . . Build that model based on the full sample. Reduce the model appropriately. Explain the model.

# Multiple regression using all predictor variables.


__$H_o$__ : coefficient  = 0, a risk of a Type 1 error, = $\alpha = 0.05$


__$H_a$__ : coefficient ≠ 0, a risk of a Type 1 error, = $\alpha = 0.05$


```{r ,  fig.width=5, fig.height=5, comment=NA}
RegModel.1 <- 
  lm(mort_5yr.num ~ rx.factor + sex.factor + age.factor + obstruct.factor + perfor.factor + adhere.factor+ differ.factor+ extent.factor,
   data=colon.data)
```
```{r}
library(sjPlot)
tab_model(
 RegModel.1 ,
  title = "Regression Model of mort_5yr.num (based on full sample) ",
  show.stat = TRUE,
  digits = 3,
  string.stat = "t-value",
  string.p = "p (sig)",
  show.fstat = TRUE,
  show.dev = TRUE,
  show.aic = TRUE,
  CSS = list(
    css.depvarhead = 'color: red;',
    css.centeralign = 'text-align: left;',
    css.firsttablecol = 'font-weight: bold;',
    css.summary = 'color: blue;'
  )
)
```


```{r plotreg, message=FALSE, echo=TRUE, fig.width=4, fig.height=5}
plotreg(RegModel.1, custom.model.names ="Regression of mort_5yr.num")
```

# Reducing the model to those variables able to predict well

There are two primary ways to reduce the size of a predictive model based on statistical analysis: 

__1__ Delete those variables that have p-values greater than 0.05; 

__2__ Delete those variables having dangerously high multicollinearity.


# Investigation of Multicollinearity (high correlation among the predictors)

</div>


Multicollinearity is the occurrence of high intercorrelations among two or more independent variables in a multiple regression model.
Multicollinearity in a multiple regression model indicates that collinear independent variables are related in some fashion, although the relationship may or may not be casual.

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

___Importance of Investigation of Multicollinearity___

</div>


Multicollinearity reduces the precision of the estimate coefficients, which weakens the statistical power of the regression model. Hence, we may not be able to trust the p-values to identify independent variables that are statistically significant. 

Before beginning the regression, the correlations below may provide some insights.
__The correlation coefficient__ indicates the strength of the linear relationship that might be existing between two variables.


# Relative Importance of Predictors (Drivers)

The table below presents all of the relative importance metrics from ‘relaimpo’. 
 
The second block gives the coefficients when 1 predictor is entered, 2 are entered through to all 11 predictors.


```{r}
library(relaimpo) 
imp.lm<-calc.relimp(RegModel.1, type=c("lmg"), rela = TRUE)
imp.lm

```


```{r}
rel.lmg<- calc.relimp(RegModel.1, type="lmg")
pm<- data.frame(rel.lmg$lmg, rel.lmg$lmg.rank)
pm<-pm[order(rel.lmg$lmg.rank),]
pm <-data.frame(Features=rownames(pm), pm, row.names=NULL)
colnames(pm) <- c("Features", "Importance", "Rankings")
is.num <- sapply(pm, is.numeric)
pm[is.num] <- lapply( pm[is.num], round, 3)
tab_df(pm)
```


The tables above answer the question of 

__What are the key drivers of mort_5yr:	5 years life status ?__


# Plot: Key Drivers of mort_5yr:	5 years life status

```{r}
theme_set(  theme_bw( ))
ggplot(pm, aes(x= reorder( Features,  Importance ), y=  Importance ) ) +
  geom_bar(stat="identity", fill="lightblue", colour="black") + 
  coord_flip() + 
  ggtitle("Key Drivers of mort_5yr:5 years life status") +
  geom_text(aes(label=format(Importance, digits=2), size=0.5, hjust=1))+
  theme(legend.position="none") +
  theme(axis.text.y=element_text(face="bold", color="black",size=12))+
  theme(plot.title=element_text(
                                face="bold", color="black",size=18))+
  theme(axis.title.x=element_text(
                                  face="bold", color="black",size=14))+
  theme(axis.title.y=element_text(
                                 face="bold", color="black",size=14))+
  labs(y= "Relative impacts of Variables", x= "Variables")

```


# Reducing the model to significant predictors


# Stepwise Regression
```{r}
library(MASS)  
step <- stepAIC(RegModel.1, direction="both")
step
```



The stepwise regression (or stepwise selection) consists of iteratively adding and removing predictors, in the predictive model, in order to find the subset of variables in the data set resulting in the best performing model, that is a model that lowers prediction error.

__Stepwise selection (or sequential replacement), which is a combination of forward and backward selections___. It starts with no predictors, then sequentially adds the most contributive predictors (like forward selection). After adding each new variable, any variables that no longer provide an improvement in the model fit are removed. (like backward selection). 
It stops when a model is obtained where all predictors are statistically significant.

The AIC at the top of each regression is the ___Akaike Information Criterion___. It compares the quality of a set of statistical models to each other. We only compare AIC value whether it is increasing or decreasing by adding more variables. Also in case of multiple models, the one which has lower AIC value is preferred.

However, AIC will choose the best model from a set, it won’t say anything about absolute quality.Therefore, once the best model has been selected, a hypothesis test will be run to figure out the relationship between the variables in the model selected and mort_5yr: 5 years life status

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

However, AIC will choose the best model from a set, it won’t say anything about absolute quality.Therefore, once the best model has been selected, __a hypothesis test__ will be run to figure out the relationship between the variables in the model selected and LifeExpectancy.

</div>



__Taking the coefficients from the so-called best model from the stepwise process and using them as the final model__

# Stating and testing the Hypothesis for the variables (rx.factor + age.factor + differ.factor + extent.factor) in the model selected (best model)

__$H_o$__: the coefficients of the variables in the the best model are equal to zero, statistically, at a risk of a Type 1 error, = $\alpha = 0.05$

__$H_a$__: the coefficients of the variables in the the best model are NOT equal to zero, statistically, at a risk of a Type 1 error, = $\alpha = 0.05$

```{r}
tab_model(step,
  title = "Regression of mort_5yr: 5 years life status on 'best' stepwise model",
  show.stat = TRUE,
  digits = 3,
  string.stat = "t-value",
  string.p = "p (sig)",
  show.fstat = TRUE,
  show.dev = TRUE,
  show.aic = TRUE,
  CSS = list(
    css.depvarhead = 'color: darkgreen;',
    css.centeralign = 'text-align: left;',
    css.firsttablecol = 'font-weight: bold;',
    css.summary = 'color: blue;'
  ))
```

# Plotting the model coefficients (the best model with select 4 variables and the original model with all 8 predictor variables)

```{r ,  fig.width=5, fig.height=5, comment=NA}
RegModel.2 <- 
  lm(mort_5yr.num ~ rx.factor + age.factor + differ.factor + 
    extent.factor,
   data=colon.data)
```
```{r mlr,  fig.width=9, fig.height=9, comment=NA}
plot_models(
  RegModel.1,RegModel.2 ,
  title = "Regression models with 4 predictor variables and with 8 predictor variables",
  show.values = TRUE,
  show.intercept = TRUE,
  show.p = TRUE,
  m.labels = c( "original model", "best model"),
  p.shape = TRUE,
  grid = FALSE,
  vline.color = "gray",
  ci.lvl = 0.95
) 
```


#The model reduced to only variables having coefficients with significant differences from zero.

```{r ,  fig.width=5, fig.height=5, comment=NA}
RegModel.2 <- 
  lm(mort_5yr.num ~ rx.factor + age.factor + differ.factor + 
    extent.factor,
   data=colon.data)
tab_model(
  RegModel.1,RegModel.2 ,
  title = "Regression models with 4 predictor variables and with 8 predictor variables",
  show.stat = TRUE,
  digits = 3,
  string.stat = "t-value",
  string.p = "p (sig)",
  show.fstat = TRUE,
  show.dev = TRUE,
  show.aic = TRUE,
  linebreak = TRUE,
  CSS = list(
    css.depvarhead = 'color: darkorange;',
    css.centeralign = 'text-align: left;',
    css.firsttablecol = 'font-weight: bold;',
    css.summary = 'color: blue;'
  )
) 
```



The RegModel.2 model above (model to the right), obtained through the stepwise regression (or stepwise selection) has the predictor variables with non-significant p-values (<0.05) that can effectively explain the response variable . 
.

Hence __RegModel.2 model__ above (model to the right) and shown in the graph below will be our __‘final’ model with predictor variables rx.factor + age.factor + differ.factor + extent.factor__.


<div class = "row">
  
<div class = "col-md-4">

<br><br> This graph shows those variables whose coefficients are significantly (p<0.05) __different from zero___ in __red__ and those __not significantly different from zero__ in __blue__.

This graph shows all variables in red, that have coefficients, significantly (p<0.05), different from zero. Since variables with coefficients not significantly different from zero have not been included hence there are no blue dots in the graph. 


</div>
  
<div class = "col-md-8">

```{r , message=FALSE, echo=TRUE, fig.width=4, fig.height=5}
plotreg(RegModel.2, custom.model.names ="Regression models of mort_5yr:5 years life status with significant predictor variables")
```


# Relative importance of the reduced model on all observations

```{r}
library(relaimpo) 
imp.model.2<-calc.relimp(RegModel.2,type=c("lmg"),rela = TRUE)
imp.model.2
```
# Relative Importance of Predictors (Drivers) in the new/final model

```{r}
rel.lmg<- calc.relimp(RegModel.2, type="lmg", rela=TRUE)
pm<- data.frame(rel.lmg$lmg, rel.lmg$lmg.rank)
pm<-pm[order(rel.lmg$lmg.rank),]
pm <-data.frame(Features=rownames(pm), pm, row.names=NULL)
colnames(pm) <- c("Features", "Importance", "Rankings")
is.num <- sapply(pm, is.numeric)
pm[is.num] <- lapply( pm[is.num], round, 3)
tab_df(pm)
```
```{r}
theme_set(  theme_bw( ))
ggplot(pm, aes(x= reorder( Features,  Importance ), y=  Importance ) ) +
  geom_bar(stat="identity", fill="darkorange1", colour="black") + 
  coord_flip() + 
  ggtitle("Key Drivers of'mort_5yr:5 years life status (best model)") +
  geom_text(aes(label=format(Importance, digits=2), size=0.5, hjust=1))+
  theme(legend.position="none") +
  theme(axis.text.y=element_text(face="bold", color="black",size=12))+
  theme(plot.title=element_text(
                                face="bold", color="black",size=18))+
  theme(axis.title.x=element_text(
                                  face="bold", color="black",size=14))+
  theme(axis.title.y=element_text(
                                 face="bold", color="black",size=14))+
  labs(y= "Relative impacts of Variables", x= "Variables")

```


# Multicollinearity in the reduced model

Variance inflation factor (VIF): The measure of multicollinearity among the independent variables in a multiple regression model.
__Table:Variance inflation factor (VIF)__ 

```{r}
library(car)
(vf <- vif(RegModel.2) )
```

The smallest possible value of VIF is 1 (absence of multicollinearity). As a rule of thumb, a VIF value that exceeds 4 indicates a problematic amount of collinearity amongst the variables.

*******************************************************************************
*******************************************************************************

# 2.a. Splitting the dataset into a training sample and a testing sample. Explaining the reason for this step and its importance to predictive modeling. 

```{r , "sample_splitting", fig.width=5, fig.height=5}
pacman::p_load(rsample)
set.seed(78)
train_test_split <- initial_split(colon.data)
train <- training(train_test_split)
test <- testing(train_test_split)
(samp <- dim(train_test_split))
```  
‘rsample’ package has split the dataset into a training sample (70% of total here) on which the model will be re-developed and a testing sample, sometimes called the holdout sample, on which the quality of predictions will be tested. 
After splitting the complete sample of `r samp[3]` subjects, `r samp[1]` were selected for the __training sample__ and `r samp[2]` for the __testing or holdout sample__.  

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

__Importance of Splitting the dataset into a training sample and a testing sample__

</div>

Cross-validation refers to a set of methods for measuring the performance of a given predictive model on new test data sets.
The basic idea, behind cross-validation techniques, consists of dividing the data into two sets:

__The training set__, used to train (i.e. build) the model;

__the testing set (or validation set)__ , used to test (i.e. validate) the model by estimating the prediction error.

The model will attempt to learn the relationship on the training data and be evaluated on the test data.
Cross-validation is primarily used in to estimate the skill of a model on unseen data. That is, to use a limited sample in order to estimate how the model is expected to perform in general when used to make predictions on data not used during the training of the model.
Overfitting and underfitting is a fundamental problem that trips up even experienced data analysts. The developed model may look great, but the problem is if a testing set is never used, the model is nothing more than an overfit representation of the training data. Thus the model does not work when someone else tries to apply it to a new data.

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

___How good is the model?___

I’ve built a model, investigated statistical properties, considered the relative importance of the predictors and reduced the model to those most important predictors that are significantly related to the response variable. Now, we need to further investigate model quality and usability.Quality of regression models can be judged in many ways. In this case it will be done by splitting the dataset randomly into a training sample (70% of total observations) on which the model will be re-developed and a testing sample, sometimes called the holdout sample, on which the quality of predictions is tested.

</div>

# 2.b. Building a regression model using the training sample data. Reducing the model to those variables having coefficients significantly different from zero. Explaining the process and the final model.

_

# Building a regression model using the training sample data.


```{r }
model.tr<-lm(mort_5yr.num ~ rx.factor + age.factor + differ.factor + extent.factor, train)
# summary(model.tr) # remove the hash-tag at the beginning of this line to see the standard summary
tab_model(
  model.tr,
  title = "Regression models of mort_5yr:5 years life status using the Training Sample",
  show.stat = TRUE,
  digits = 3,
  string.stat = "t-value",
  string.p = "p (sig)",
  show.fstat = TRUE,
  show.dev = TRUE,
  show.aic = TRUE,
  CSS = list(
    css.depvarhead = 'color: red;',
    css.centeralign = 'text-align: left;',
    css.firsttablecol = 'font-weight: bold;',
    css.summary = 'color: blue;'
  )
)
```


# Stating and testing the Hypothesis for the variables in the __"training_model"__

__$H_o$__: the coefficients of the variables in the the training_model are equal to zero, statistically, at a risk of a Type 1 error, = $\alpha = 0.05$

__$H_a$__: the coefficients of the variables in the the training_model are NOT equal to zero, statistically, at a risk of a Type 1 error, = $\alpha = 0.05$
 

# Reducing the model, using Stepwise Regression, to those variables having coefficients significantly different from zero

Since the variable 'Rural' has coefficient that __is  significantly not different from zero (p (sig) = 0.058 > 0.05)__. Hence __$H_o$__ cannot be rejected in this case. 

The stepwise regression was used to find the "best training model" with subset of predictor variables that are statistically significant.

```{r}
library(MASS)  
step <- stepAIC(model.tr, direction="both", trace = TRUE)
step
```
In case of current stepwise regression for LifeExpectancy using the Training Sample, the model with variables 'BirthRate', 'GDP' , 'Health' , 'HIV' , 'Rural' has been chosen as the best training model. 
The “plotreg()” function is used below to present the coefficient estimates based on the training sample. The graphs show that, while the intercept is not significantly different from zero, all the five predictor variables significantly help to improve prediction of LifeExpectancy. 
```{r}
plotreg(model.tr, custom.model.names =" Regression models of 'LifeExpectancy' using the Training Sample")
```

# 2.c. Reporting on the relative importance of those variables that remain in the model with a table, graph and in words. 

```{r}
library(relaimpo) 
imp.model.3<-calc.relimp(model.tr,type=c("lmg"),rela = TRUE)
imp.model.3
```

```{r}
rel.lmg<- calc.relimp(model.tr, type="lmg", rela=TRUE)
pm<- data.frame(rel.lmg$lmg, rel.lmg$lmg.rank)
pm<-pm[order(rel.lmg$lmg.rank),]
pm <-data.frame(Features=rownames(pm), pm, row.names=NULL)
colnames(pm) <- c("Features", "Importance", "Rankings")
is.num <- sapply(pm, is.numeric)
pm[is.num] <- lapply( pm[is.num], round, 3)
tab_df(pm)
```


___lmg calculates the relative contribution of each predictor to the R square with the consideration of the sequence of predictors appearing in the model___.

Using the 'lmg' metric, the tables above answer the question of __What are the key drivers of mort_5yr:5 years life status in the training model__.

___Plot: Key Drivers of mort_5yr:5 years life status___

The key drivers of variable 'LifeExpectancy' can be visually represented using the following diagram. 

```{r}
theme_set(  theme_bw( ))
ggplot(pm, aes(x= reorder( Features,  Importance ), y=  Importance ) ) +
  geom_bar(stat="identity", fill="lightgreen", colour="black") + 
  coord_flip() + 
  ggtitle("Key Drivers of mort_5yr:5 years life statusin Training Model") +
  geom_text(aes(label=format(Importance, digits=2), size=0.5, hjust=1))+
  theme(legend.position="none") +
  theme(axis.text.y=element_text(face="bold", color="black",size=12))+
  theme(plot.title=element_text(
                                face="bold", color="black",size=18))+
  theme(axis.title.x=element_text(
                                  face="bold", color="black",size=14))+
  theme(axis.title.y=element_text(
                                 face="bold", color="black",size=14))+
  labs(y= "Relative impacts of Variables", x= "Variables")

```


# 2.d. Reporting on the residuals and test if the distribution of the residuals is normal. Why is this important? 

# The residuals
```{r}
predicted <- predict(model.tr, newdata=test  )
actual <- test$mort_5yr.num # actual Q3a attitudes for testing sample
residual <- predicted - actual   
x<- as.data.frame(cbind(actual, predicted, residual)) 
```
```{r}
library(kableExtra)
head(x, 10) %>%
  kable("html", align = 'clc', digits=2, col.names=c( "actual", "predicted", "residuals")) %>%
    kable_styling(bootstrap_options = "striped", full_width = F, position = "left") %>% 
    footnote(general = "The actual and predicted values of mort_5yr:5 years life status and the residuals for the first 6 Patients.")
```



# Testing if the distribution of the residuals is normal

__Stating Null and Alternative Hypothesis:__ 

$H_o$: Distribution of residuals is Normal , risk of a Type 1 error = $\alpha = 0.05$.   

$H_a:$ Distribution of residuals is not Normal

```{r  fig.width=5, fig.height=5 }
library("car")
qqPlot(model.tr, main="QQ Plot") 
```


The plot above shows the distribution of the residuals against the expected normal distribution. For normally distributed data, observations should lie approximately on the reference line. Since, there are some (approximately 4) outliers in the data (points at the ends of the line distanced from the bulk of the observations and the reference line) the residuals may not be normally distributed. Hence, further investigation is required. 

*******

In the density plots below, we attempt to visualize the underlying probability distribution of the residuals by drawing an appropriate continuous curve. 
Residuals do not seem to be normally distributed as residuals are not symmetric about the mean and show bimodality. It should be further investigated by the conducting Normality testing.

```{r fig.height=5, fig.width=5, message=FALSE, warning=FALSE, comment=NA, include=FALSE}
d <- densityPlot(residual)
```
```{r echo=TRUE, message=FALSE, warning=FALSE, fig.width=5, fig.height=5, comment=NA}
plot(d, main=" Density Plot Residuals")
polygon(d, col="orange", border="green")
```

__Testing the hypothesis using Shapiro-Wilk’s method__

Shapiro-Wilk’s method has been used to test the normality of Residuals, considering a significance level of 0.05. 

```{r echo=TRUE, message=FALSE, warning=FALSE, fig.width=5, fig.height=5, comment=NA}
shapiro.test((residual))
```



# How Important Are Normal Residuals in Regression Analysis?

Prediction intervals are calculated based on the assumption that the residuals are normally distributed. If the residuals are non-normal, the prediction intervals may be inaccurate, affecting the reliability and usability of the model.
Moreover, while building a optimally fitting linear model, it is assumed that the relationship is linear, and that the errors, or residuals, are simply random fluctuations around the true line. If residuals are not (approximately) normally distributed (with a mean of zero), it can pinpoint potential issues with the model.

## 2.e. Testing homoscedasticity and Stating hypotheses. 

The assumption of homoscedasticity (meaning “same variance”) is central to linear regression models.  Homoscedasticity describes a situation in which the error term (that is, the “noise” or random disturbance in the relationship between the independent variables and the dependent variable) is the same across all values of the independent variables. 

Homoscedasticity in the best model  is tested below using the Breusch Pagan Test. It tests whether variances of residuals from a regression are dependent on the values of an independent variable.

__Stating Null and Alternative Hypothesis:__ 

$H_o:$ there are equal or constant variances,  $\alpha$ = 0.05

$H_a:$ there are unequal or non-constant variances    

```{r}
pacman::p_load( "olsrr" )
ols_test_breusch_pagan(RegModel.2)
```



# 2.f. Using the model based on the training sample to predict mort_5yr:5 years life status for those in the testing sample. How good is the model based on the training sample when used on the testing sample data? 

## Predicting mort_5yr:5 years life status using the training model on the testing dataset

“predict()” evaluates the model equation on values of rx.factor + age.factor + differ.factor + extent.factor for each respondent in the testing sample. While the model was based on those respondents in the training sample, the respondents in the testing sample did not contribute to those calculations and can be considered to be “fresh” or “new” subjects.

__The code-chunk below predicts the mort_5yr:5 years life status for the test, or holdout, sample of patients using the model built on the training data.__

```{r}
model.te<-lm(mort_5yr.num ~ rx.factor + age.factor + differ.factor + extent.factor, test)
predicted.tr.te <- predict(model.tr, newdata=test  )
predicted.tr.te
actual <- test$ mort_5yr.num # actual attitudes for the testing sample
x<- as.data.frame(cbind(actual, predicted.tr.te)) 
```

### Assessing the model based on the training sample when used on the testing sample data. 

#### Scatter plots of Predicted vs Actual

```{r,  fig.width=5, fig.height=5}
scatterplot(x$predicted~x$actual, regLine=TRUE, smooth=FALSE, boxplots=FALSE, data=colon.data,col = "red", main = "Scatterplot of Predicted vs Actual " ,xlab = "Actual",ylab = "Predicted")
```

Scatter plots of Predicted vs Actual is used to understand how well the regression model makes predictions for different response values.  A perfect regression model has a predicted response equal to the true response, so all the points lie on a diagonal line. The vertical distance from the line to any point is the error of the prediction for that point. A good model has small errors, and so the predictions are scattered near the line.

Hence, it may be concluded that the model has a weak Goodness of fit since most of the points are foggy or dispersed (away from the diagonal line). 

The analysis above, further mandates the assessment of goodness of the model based on the training sample when used on the testing sample data. 

# Correlation between actual and predicted values of the dependent variable, Life Expectancy

___Stating the Hypothesis___

Ho: correlation = 0 i.e., actual and predicted values of the dependent variable, Life Expectancy, are __statistically independent__, a risk of a Type 1 error, = α=0.05

Ha: correlation ≠ 0 i.e., actual and predicted values of the dependent variable, Life Expectancy, are __statistically correlated__ , a risk of a Type 1 error, = α=0.05 are correlated a risk of a Type 1 error, = α=0.05


___Testing Hypothesis___


```{r,  fig.width=5, fig.height=5}
( c<- cor.test(x$actual,x$predicted))
library(sjPlot)
tab_corr(x, digits=2, show.p=TRUE, p.numeric=TRUE)
```

___Interpretations___

From the analysis above, it can be inferred that actual and predicted values of the dependent variable, Life Expectancy, are significantly correlated (r = 0.9052694 , p = 0.00000009964). Since p = 0.00000009964 < 0.05, the null hypotheses can be rejected. Also, the confidence interval does not include 0.

# Global test of model assumptions using the ‘gvlma’ package

The ‘gvlma’ package provides a summary assessment of the assumptions for a linear model and they are below for the testing model.
```{r,  fig.width=5, fig.height=5}
pacman::p_load(gvlma)
gvmodel <- gvlma(model.te) 
summary(gvmodel)
```



___Interpretations___

The summary above provides the following information: 

The intercept with a value of 76.57593133 is well above the origin i.e., the regression line does not pass through the origin.

GDP & Health variables have positive coefficients which indicate direct impact on the dependent variable, LifeExpectancy, whereas HIV, Rural and BirthRate have negative coefficients which indicate inverse relationship with the dependent variable, LifeExpectancy. 

The t-statistic/t-value is just the estimated coefficient divided by its own standard error. Thus, it measures “how many standard deviations from zero” the estimated coefficient is.

From the p value of 0.000002631 we can estimate that statistically intercept goes beyond zero and can reject the null hypotheses that the line intercepts the vertical axis at the origin, α = 0.05.

Also , from the p value of  0.000002631 we can estimate that statistically coefficient of correlation is not equal to zero. Hence, at a risk of a Type 1 error, = α=0.05 and p value <0.05, we can reject the Ho that correlation = 0 and conclude that correlation ≠ 0 i.e., 

The regression model is  very well able to predict the dependent variable, Life Expectancy	[Average life expectancy (years)]  

Moreover, The ‘gvlma’ test provides a lot of output for the regression and then states whether 5 assumptions are acceptable or not. In this case, all assumptions pass the tests except for the link function which indicates that we should use an alternative form of the generalized linear model (e.g. logistic or binomial regression).














********************************************************************
*************************************************************
<hr />
<p style="text-align: center;">Work done by Sandeep Kumar</p>
<p style="text-align: center;">MSc eHealth Program</p>
<p style="text-align: center;">McMaster University</p>
<p style="text-align: center;"><span style="color: #808080;"><em>kumars53@mcmaster.ca</em></span></p>
&nbsp; 




