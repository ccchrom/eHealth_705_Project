---
title: "eH705 Final Project"
subtitle: "Chemotherapy for Stage B/C Colon Cancer"
author:
  name: Neha Kalra, Matthew Jabora, Ibrahim Zifa, Sandeep Delar, Romuald Lamps 
  affiliation: eH705 | eHealth MsC
date: "`r format(Sys.time(), '%d %B %Y')`"

output:
  html_document: 
    number_sections: TRUE
    code_folding: hide
    toc: true
    toc_float: true
    toc_depth: 4
    toc_collapsed: true
    theme: readable
--- 

```{r setup, eval=TRUE, include=FALSE}  
knitr::opts_chunk$set( echo=TRUE, warning=FALSE, message=FALSE, error=FALSE, comment = NA )
options(max.print = 10000)
options(scipen = 999)
pacman::p_load(DT, kableExtra, magrittr, ggplot2, wrapr, generics, car, psych,xray , ggplot2, texreg, dplyr, wrapr,  sjPlot, knitr, DescTools, Hmisc, corrplot, ggcorrplot, qgraph, corrr, tidyverse,sjmisc, sjlabelled, sjstats, sjPlot,  forcats, kableExtra, captioner, unikn, captioner, car, hrbrthemes, relaimpo, plotflow, tidyverse, dials, tidymodels, ggeffects, haven, glmmTMB, broom, magick, BiocManager, vtree, pscl)
remotes::install_github('rstudio/rmarkdown')
```   
```{r "colorize", echo=FALSE, results="hide"}
colorize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color, 
      x)
  } else x
}
```  

# Objectives 

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">
We wish to confirm using our own process that adjuvant therapy with the combination of levamisole and fluorouracil significantly reduce death within 5 years compared with no adjuvant therapy – using only levamisole.  We chose to only include these two groups because we are only comparing adjuvant therapy against normal therapy.  Levimasole is already recognized worldwide as an antiparasitic drug that changes ones immune response, similar to methotrexate – so we are considering it a control.  We aim to achieve this by:<br>

1.  Assessing the distributions of variables within the two treatment categories, including: (distribution plots/tables between groups)  
    a.	Sex – Male or female patients  
    b.	Age – Finding the average (or median) age that has most impact on 5 year survival  
    c.	Obstructive tumours versus non-obstructive  
    d.	If the colon was perforated by the tumor or not  
    e.	If tumors were adhered to nearby organs  
    f.	Differentiation of tumors – rated well, moderate, or poor  
    g.	The extent of local spread of the tumor which could be:  
        i.	Submucosal  
        ii.	Within muscle  
        iii.	Within the serosa – the outermost layer of organs  
        iv.	Contiguous structures – meaning nearby organs  
2.  Investigating if any of the predictor variables have an impact on each other. (correlation)(VIF)  
3.  Using stepwise (linear) regression to obtain a model that best describes the impact of specific variables on time until death.  
4.	Using a logistic regression model to investigate the association between five year life status and one or more categorical predictor variables within each treatment group (regression models for both groups) (predict) (train+test) (residuals)  
5.	Assessing whether there are differences in survival (or cumulative incidence of event) among different groups of participants  
    a.  This will be done by testing the null hypothesis of no difference in survival between two or more independent groups.  
        i.	H0: The two survival rates are identical, (α=0.05).    
        ii.	Ha: The two survival rates are not identical.  (chi-square test)  
6.	Analyzing data within the subset of Levamisole-fluorouracil to find which variables have the greatest impact on 5 year mortality.  (Reliampo)  
</div>

# Read the file  
<div class = "blue">
This data comes from a study investigating the addition of fluorouracil to the treatment regime of Levamisole in subjects with colon cancer.<br>
Moertel, C., Fleming, T.,Macdonald, J., Haller, D., Laurie, J., Goodman, P., Ungerleider, J., Emerson, W., Tormey, D., Glick, J., Veeder, M. & Mailliard, J. (1990). Levamisole and Fluorouracil for Adjuvant Therapy of Resected Colon Carcinoma. <i> The New England Journal of Medicine. </i> 332(<i>6</i>).
</div>

```{r}
colon.data <- read.csv('colon_s.csv')
str(colon.data)
```

# Investigating, Cleaning and Recoding the Data  
## Column descriptions  

The description of the variables for this dataset are listed below.

```{r}
df = as.data.frame(matrix( 
  c('id','id',
    'rx','Treatment - Obs(ervation), Lev(amisole), Lev(amisole)+5-FU',
    'sex','sex of the patient (1 = Male, 0 = Female)',
    'age','age in years',
    'obstruct','obstruction of colon by tumour',
    'perfor','perforation of colon',
    'adhere','adherence to nearby organs',
    'nodes','number of lymph nodes with detectable cancer',
    'status','censoring status',
    'differ','histological differentiation of tumour (1=well, 2=moderate, 3=poor)',
    'extent','Extent of local spread (1=submucosa, 2=muscle, 3=serosa,
4=contiguous structures)',
    'surg','time from surgery to registration (0=short (7-20 days), 1=long (21-35 days))',
    'node4','more than 4 positive lymph nodes',
    'time','days until death',
    'loccomp','location of primary tumor - mi;to[;e ',
    'time.years','years until death',
    'mort_5yr','5 years life status',
    'age.10','Age divided by 10..',
    'hospital','Hospital'), # the data elements 
  ncol = 2,              # number of rows 
  byrow = TRUE))
colnames(df) <- c("Variable", "Description")
df %>%
  kable(caption = "World Population Health: All Countries") %>%
  kable_styling( bootstrap_options = "striped", full_width = F, position = "left", fixed_thead = T)
```

## Overview of Data
<div class = "blue">
Below tree diagram shows:

* Total number of observations- 929
* 33% of the observations are for patients who were given levamisole (Lev).
* 33% were given levamisole + fluorouracil (Lev +5FU)
* 34% constitutes the controls group. *Note: the control group data will be deleted below*
* There is almost equal distribution of male and females in both the groups (Lev and Lev+5FU)
* Majority of the patients belong to the age group of 60+ followed by 40-59 and <40 years.
</div>

```{r echo=FALSE, , fig.align="center", fig.width=6, fig.height=6, fig.cap="Figure 2:add"}
library(vtree)
vtree(colon.data, c("rx.factor", "sex.factor", "age.factor"), horiz = FALSE, fillcolor = c(restecg = "#e7d4e8", cp = "#99d8c9"), keep = list(restecg = c("ST-T abnormality")))
```

## Converting Variables 
<div class = "blue">
Below, character variables are converted to factors, integers are converted to numbers, and the observation arm is filtered out.  This is done to assist with our analysis.  The removal of the observation group done to primarily focus on the comparison between adjuvant therapy and the current standard at that time, and to also focus on other factors - aside from treatment - that may effect survival within that group.
</div>
```{r}
library(dplyr)
colon.data <- colon.data %>% filter(rx!="Obs")
colon.data <- colon.data %>% mutate_if(is.character, as.factor)
colon.data <- colon.data %>% mutate_if(is.integer, as.numeric)
colon.data$mort_5yr <- fct_relevel( (colon.data$mort_5yr) , c("Died", "Alive" ) )
library(Hmisc)
describe(colon.data)
```

## Missing Values and Treatment

In this section, the total count of missing values in each column is identified and further treated by replacing them with the most frequently occurring value for that variable. *Note* - Zeros are not treated as part of treating missing values as zeros are one of the valid response outputs.

### Total Count of Missing Values>
<div class = "blue">
The summary table below provides total count of NA for all the variables along with other information like minimum value, Max.value, Mean, Median etc. From the table below it is identified 6 of the variables contain NAs that need to be treated for further analysis.
</div>
```{r}
MD <- colSums(is.na(colon.data))
library(kableExtra)
MD[c(3:14,27,29:30)] %>%
  kable(caption = "Table 3: Total count of NA for each variable") %>%
  kable_styling( bootstrap_options = "striped", full_width = F, position = "left", fixed_thead = T)
```
<div class = "blue">
It is noted that within this dataset, that there are already factor versions of the variables.  To be sure not to double count, below is a summary of the NAs:

* Below are the columns identified with NA:
   * obstruct - 15
   * nodes - 15
   * differ - 16
   * surg - 11
   * loccomp - 14
   * mort_5yr - 8
   
The total of NAs is 79 - which is roughly 12% of the filtered sample of 614.  We chose to omit these values due to the large sample size.  There was worry that if we imputed this data - which is mostly two-level factors - that there may be a large margin of error.  
</div>

```{r}
library(dplyr)
colon.data <- colon.data %>%
filter(rx!="Obs") %>%
na.omit()
```

# Investigating the Cleaned Data
## Visual Representation 
The below table and vtree give a quick glance at the cleaned data
```{r}
colon.data %>% datatable()
```
<div class = "blue">
Below tree diagram shows:

* Total number of observations in the Cleaned Dataset- 614
* 50% of the observations are for patients who were given levamisole (Lev).
* 50% were given levamisole + fluorouracil (Lev +5FU)
* In "Lev" treatment group, majority of the patients belong to male whereas in "Lev +5FU" treatment group, majority of the patients belong to Females.
* Majority of the patients belong to the age group of 60+ followed by 40-59 and <40 years.
</div>

```{r echo=FALSE, , fig.align="center", fig.width=6, fig.height=6, fig.cap="Figure 2:add"}
library(vtree)
vtree(colon.data, c("rx.factor", "sex.factor", "age.factor"), horiz = FALSE, fillcolor = c(restecg = "#e7d4e8", cp = "#99d8c9"), keep = list(restecg = c("ST-T abnormality")))
```

## Preliminary Statistical Findings
<div class = "blue">
Below are the preliminary findings after reading the graphs and reports produced by the method used above.

The cleaned, recoded sample has total of 551 observations.

Some preliminary observations for the variables used in the analysis below:  
    * Sex - A fairly equal distribution between Male and Female.  
    * Treatment - 15 more observations in the Levimasole group.    
    * Obstruction - Majority of the observations have no obstructive tumors.  
    * Perforation - Majority of the observations is "No" while a very few observations belong to "Yes".  
    * Adherence - Majority of the observation is "No" for adherence to nearby organs.  
    * Differ - Majority of the observations belong to the "Moderate" histological differentiation category.  
    * Extent - Majority of the observation belongs to tumor growth into the serosa (lining of the intestine).  
    * Time to Treatment (surg) - Majority of observations had a shorter time to treatment - meaning within 7 to 20 days.  
    * Nodes - Majority of observations had less than 4 lymph nodes affected by the tumor.  
    * Time.years - The mean of years until death is 4.6.  
    * Age Group - Majority of the observation belongs to 60+ age group. 
    * Mort_5 yr - Majority of the observations belong to "Alive".  
</div>
```{r echo=TRUE, message=FALSE, error=FALSE, warning=FALSE}
summary(colon.data)
```


### Demographic Distribution of the data
The graphs below show the demographic information between our treatment groups.

```{r}
library(dplyr)
df <- colon.data %>%
  count(sex.factor, rx.factor) %>%
  mutate(pct = n / sum(n),
         pct_label = scales::percent(pct))
library(ggplot2)
ggplot(df, aes(x= sex.factor, fill = rx.factor, y = pct)) +
  geom_col() +
  geom_text(aes(label = paste(pct_label, n, sep = "\n")), 
                lineheight = 0.8,
                position = position_stack(vjust = 0.5)) +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_brewer(palette="Blues", name = "Treatment stacked by sex")
```


```{r}
library(dplyr)
dx <- colon.data %>%
  count(age.factor, rx.factor) %>%
  mutate(pct = n / sum(n),
         pct_label = scales::percent(pct))
library(ggplot2)
ggplot(dx, aes(x= age.factor, fill = rx.factor, y = pct)) +
  geom_col() +
  geom_text(aes(label = paste(pct_label, n, sep = "\n")), 
                lineheight = 0.8,
                position = position_stack(vjust = 0.5)) +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_brewer(palette="Blues", name = "Treatment stacked by age group")
```


#### Findings

<div class = "blue">

* There are almost equal distribution of males and females in the study. There are more females on the Lev+FU treatment.

* Majority of the study population is from +60 age group followed by 40-59 years and a very small propoortion belong to <40 years age group. There is approximately equal distribution of the population in the 3 age group categories for both the treatments.


</div>

### Impacts of the treatments on the Prognostic Markers of Colon Cancer

Below graphs are plotted to understand the distribution of tumor descriptors and different outcomes for each treatment.   

```{r echo=TRUE, message=FALSE, warning=FALSE, error=FALSE, fig.width=8, fig.height=8, comment=NA}
library(ggplot2)
p01 <- ggplot(data = colon.data)+ geom_bar (aes(x=perfor.factor, fill=(rx.factor)))+
       ggtitle(label = "Perforation of colon") +
  scale_fill_brewer(palette="Blues", name = "Treatment")
       
p02 <- ggplot(data = colon.data)+ geom_bar (aes(x=obstruct.factor, fill=(rx.factor)))+
       ggtitle(label = "Obstruction of colon by tumour") +
  scale_fill_brewer(palette="Blues", name = "Treatment")
p03 <- ggplot(data = colon.data)+ geom_bar (aes(x=adhere.factor, fill=(rx.factor)))+
       ggtitle(label = "Adherence to nearby organs") +
  scale_fill_brewer(palette="Blues", name = "Treatment")
p04 <- ggplot(data = colon.data)+ geom_bar (aes(x=differ.factor, fill=(rx.factor)))+
       ggtitle(label = "Differentiation of tumour") +
  scale_fill_brewer(palette="Blues", name = "Treatment")
p05 <- ggplot(data = colon.data)+ geom_bar (aes(x=extent.factor, fill=(rx.factor)))+
       ggtitle(label = "Extent of local spread") +
  scale_fill_brewer(palette="Blues", name = "Treatment")
p06 <- ggplot(data = colon.data)+ geom_bar (aes(x=mort_5yr, fill=(rx.factor)))+
       ggtitle(label = "5 years life status") +
  scale_fill_brewer(palette="Blues", name = "Treatment")
p07 <- ggplot(data = colon.data)+ geom_bar (aes(x=age.factor, fill=(rx.factor)))+
       ggtitle(label = "Age group") +
  scale_fill_brewer(palette="Blues", name = "Treatment")
p08 <- ggplot(data = colon.data)+ geom_bar (aes(x=node4.factor, fill=(rx.factor)))+
       ggtitle(label = "More than 4v nymph nodes") +
  scale_fill_brewer(palette="Blues", name = "Treatment")
pacman::p_load("Rmisc")
multiplot(p01,p02,p03,p04, cols=1)
multiplot(p05, p06, p07, p08, cols =1)
  
``` 

#### Findings

<div class = "blue">
All of these charts show visually, that there is a fairly even distribution within the two treatment groups.  They also show that, within this sample:  
    * Almost all tumors did not perforate the colon.  
    * Majority of tumors did not obstruct the colon.  
    * Majority of tumors did not adhere to nearby organs.  
    * Majority of tumors had a histological differentiation of moderate.  
    * Majority of the extent of localized tumor spread was to the serosa (lining) of the colon.  
    * Slightly more subjects were alive at the five year mark.
    * Majority of the age within the sample was over 60 years old.  
    * Majority of the tumors did not effect more than four lymph nodes.  
</div>

# Splitting into Training and Testing Samples
<div class = "blue">
In order to do further analysis, we have to create training and testing samples  The training sample will be used to create an equation to show the impact that tumor characteristics and treatment will have on life expectancy.  The test sample is used to test that equation for it's accuracy.

```{r , "sample_splitting", fig.width=5, fig.height=5}
pacman::p_load(rsample)
set.seed(78)
train_test_split <- initial_split(colon.data)
train <- training(train_test_split)
test <- testing(train_test_split)
(samp <- dim(train_test_split))
```  

After splitting the complete sample of `r samp[3]` subjects, `r samp[1]` were selected for the training sample and `r samp[2]` for the testing (or holdout) sample.  
</div>
# Logistic Regression using Five Year Mortality  
For logistic regression, we must use a factor as the response variable.  We opted to use mort_5yr as it provides inside into whether a subject was alive or deceased at that time.  The hope here is to see which factors have the biggest impact on mortality - treatment, or which tumor descriptors?

## NULL model 
The NULL model is created in order to compare our other models to it and assess for its accuracy.  It is the starting point in which we aim to lower the deviance and AIC.  The frequency distribution of mortality at five years is provided below. Notice that 57.71% of the patients are alive after 5 years. 
```{r}
frq( colon.data$mort_5yr , out="v", title= "Sample distribution between dead and alive patients")
```
<br>
<div class = "blue">
The Null model is related to the null hypothesis. For a logistic regression model like: 
$Probability(mortalityAt5years) = f( constant + \beta_1 * X_1 + \epsilon)$, the hypotheses are:

<p style="border:3px; border-style:solid; border-color:#2B547E; padding: 1em;">
$H_o: \beta_1 = 0$, $\alpha = 0.05$  
$H_a: \beta_1 \ne 0$ 
</p>

If the null hypothesis could not be rejected, the equation would be: $Probability(mortalityAt5years) = f( constant + \epsilon )$, where epsilon represents random error.

The code-chunk below sets up the null model as "mort_5yr ~ 1" and shows its "Null deviance" of 750.68, which measures the variation among values of the response variable. That deviance becomes a basis of comparison for all other models, as well as the AIC (Akaike Information Criterion - a measurement of the accuracy of the model). If a model can't significantly beat the Null model by producing a lower deviance and AIC, it is not worth pursuing.  
</div>
```{r }
# the Null model is
null_model <- stats::glm(mort_5yr ~ 1, family=binomial(logit), data = colon.data)
summary(null_model)
```  

The odds ratio is `r coef(null_model) %>%  exp()` as can be obtained by using the following code.

```{r}
coef(null_model) %>%  exp()
```  
<div class = "blue">
The odds of "Alive after 5 years" is $\frac{318}{233}$ = `r 318/233` from the frequency table above, which is the same as the odds ratio of the model above. This really just represents informed guessing. If you knew that 57.71% of the sample will be alive after 5 years and you were asked to guess if a succession of patients are alive or dead, you would tend to guess "alive" slightly often than "died". You should attempt to adjust your guessing so that "alive" is mentioned $\frac{318}{233} - 1$ = 36.48% more often than "died".

Of course, we will need some way to test if the model using a predictor variable is any better than the Null model. We will return to the Null model later and use it as a comparison to see if other models improve on the Null.
</div>
## Use All Variables to Perform Logistic Regression   
<div class = "blue">
The variables: "extent of local spread", "age" group and "more than 4 positive lymph nodes" seem to be the main predictors of the 5 year life status. 
</div>
```{r}
#  + surg.factor + node4.factor + loccomp.factor
model_reg_all <- glm(mort_5yr ~ extent.factor + age.factor  + node4.factor + obstruct.factor + perfor.factor + adhere.factor + differ.factor + surg.factor + nodes + loccomp.factor + rx.factor + sex.factor + age, family= binomial(logit), data = colon.data)
summary(model_reg_all)
```

## Create a Reduced Model
<div class = "blue">
Using the variables shown to have an impact above - extent, age, and nodes4 - we create a new model to check its deviance and AIC.  The deviance of this model is 664, lower than the NULL deviance, and its AIC (678.64) is also lower than the NULL deviance.  This shows that by focusing on these 3 variables, and not the treatment, we are able to predict mortality more accuratly.
</div>

```{r}
model_reg_extent_age_node4 <- glm(mort_5yr ~ extent.factor + age.factor + node4.factor, family= binomial(logit), data = colon.data)
summary(model_reg_extent_age_node4)
```

### Multicollinearity
<div class = "blue">
Multicollinearity is testing to assess whether these variables have an ability to impact each other, rather than solely the response variable - mort_5yr.  When doing this in logistic regression, you test using Variance Inflation Factors (VIF).  A rule-of-thumb says that if the VIF is greater than 4, there may be a severe multicollinearity problem and when above 10 multicollinearity problems are very likely.  The VIF below are all low and are of no concern.
</div>
```{r}
pacman::p_load(car)
(vf <- vif(model_reg_extent_age_node4) )
```

## Compare the 3 models
<div class = "blue">
Adding all input as predictors will give the best deviance, but it will be overfitting - allowing for errors to effect the equation. By only including the three significant predictors (as determined by their p-value), the deviance increases slightly, but the AIC decreases. The AIC is a measure adjusted to penalize the inclusion of a variable that doesn't benefit the model. 

**So we can conclude that the extent of the spread, the age group and 'more than 4 positive lymph nodes' are the main predictors of the life status at 5 years within this data.**

But they are particularly efficient at determining the mortality at 5 years. The $R^2$ indicates that only 14.8% of the mortality at 5 years can be explained by the extent of the spread, the age group and "more than 4 nodes" factor.
</div>

```{r}
tab_model(  model_reg_all, model_reg_extent_age_node4, null_model, show.p=TRUE,  show.r2=TRUE, show.stat = TRUE, show.aic = TRUE, show.dev = TRUE)
```

## ANOVA  
<div class = "blue">
Here we use ANOVA to check if the chosen model deviance is significantly different from the deviance of the NULL model.  The null hypothesis is that the deviances are the same.

<p style="border:3px; border-style:solid; border-color:#2B547E; padding: 1em;">
$H_o: \mu_1 = \mu_2 = \mu_3 = ...$ risk of a Type 1 error = $\alpha = 0.05$ or something less.  
$H_a:$ at least one of the means is different from the others.
</p>

The below ANOVA test indicates that it is significantly different ( p = 0.00000000000000022 < 0.05)
</div>
```{r}
anova(null_model, model_reg_extent_age_node4, test ="Chisq")
```

## Predicting Against Testing Sample
Prior to using the above equation, we must ensure that it is suitable for our training sample first.

### Training model
Before we assess the correct equation based off of using the entire sample, it helps to look at the Null model, as well as the full model for the training sample.

#### The "NULL" Model Using the Training Sample

The NULL model for the training model is shown below:

```{r}
null_model_train <- glm(formula = mort_5yr ~ 1, 
    family = binomial(logit), data = train)
summary(null_model_train)
```

#### The Full Model Using Training Sample

Similar to the model generated from the full sample, the extent of the spread, the age group and the node4 appear to be the main predictors of the life status at 5 years.

```{r}
model_reg_all_train <- glm(mort_5yr ~ extent.factor + age.factor  + node4.factor + obstruct.factor + perfor.factor + adhere.factor + differ.factor + surg.factor + nodes + loccomp.factor + rx.factor + sex.factor + age, family= binomial(logit), data = train)
summary(model_reg_all_train)
```
Now we look at the equation's effectiveness within the training sample.  
```{r}
training_model <- glm(mort_5yr ~ extent.factor + age.factor + node4.factor, family= binomial(logit), data = train)
summary(training_model)
```
<div class = "blue">
**The selected predictors from the full sample appear to also be significant with the training model.**

The intercept represent the 'alive' odds of a patient with the following characteristics:  
  * The extent of local spread of the tumor is "Contiguous structures – meaning nearby organs"  
  * The age group is '<40 years'    
  * With less than 4 positive lymph nodes  

The odds being alive for this patient is $e^{-2.5156}$  = `r exp(-2.5156)`

The odds will increase to  $e^{-2.5156 + 3.2343}$ = `r exp(-2.5156 + 3.2343)` if the extent of the spread is submucosa., but will decrease further to $e^{-2.5156 -1.3925}$ = `r exp(-2.5156 +-1.3925)` if the patient has more than 4 positive lymphe nodes.
</div>
#### McFadden $R^2$
<div class = "blue">
The McFadden $R^2$ is often considered to be the most credible metric to indicate the approximate percentage of variance in life status that’s explained by the extent of the spread, the age group and the number of nymph nodes. However, its value of 11.95% is very modest.
</div> 
```{r}
M_r2 <- pR2(training_model)
M_r2
```

#### odds ratios and 95% Confidence Interval

```{r}
tab_model(
  training_model,
  title = "Logistic Regression Odds Ratios for life status at 5 years using the Training Sample",
  show.stat = TRUE,
  digits = 3,
  string.stat = "z-value",
  string.p = "p (sig)",
  show.fstat = TRUE,
  show.dev = TRUE,
  show.aic = TRUE,
  CSS = list(
    css.depvarhead = 'color: red;',
    css.centeralign = 'text-align: left;',
    css.firsttablecol = 'font-weight: bold;',
    css.summary = 'color: blue;'
  )
)
```


```{r}
pacman::p_load(ggplot2)
plot_model(
  training_model,
  title = "Logistic Regression of life status at 5 years using the Training Sample",
  show.p = TRUE
)
```

#### Using the Model

The model coefficients can be used to predict the 'alive' probability a person with a given tumor extent, of a particular age group and having "more than 4 positive lymph nodes" or not.

The first 6 people in the training sample are shown below.

```{r}
head(train[, c(2, 22, 26, 24, 30)]) %>% datatable(  rownames = F, filter="top", options = list(pageLength = 10, scrollX=T))
```
<div class = "blue">
Probability of being alive if you’re the person with RID = 2 above whose spread of the tumor is 'serosa', is over 60 years old and has less than 4 nymph nodes calculates as:
$e^{-2.5156 + 1.7759 + 1.2298}$ = `r exp(-2.5156 + 1.7759 + 1.2298)`,which is a probability of `r exp(-2.5156 + 1.7759 + 1.2298) / (1+exp(-2.5156 + 1.7759 + 1.2298))`

The probabilities of being alive for the first 6 subjects are below:

```{r}
training_model$ fitted.values[1:6]
```

These probabilities can be compared with the actual life status of each patient:
</div>
```{r}
probs <- training_model$ fitted.values[1:6]
mort_5yr_model <- ifelse(probs > 0.5, "Alive", "Died")
head(cbind(train[, c(2, 22, 26, 24, 30)],mort_5yr_model)) %>% datatable(  rownames = F, filter="top", options = list(pageLength = 10, scrollX=T))
```

## Predicting Using the Training Model on the Test Sample
### Hit Ratio
The Hit Ratio tests the accuracy of an equation to get the same results on the test sample, as it did on the training sample.  
<div class = "blue">
The Hit ratio is $\frac{65+27}{137}$ or 67.15%. This is not a good model as we would hope to reach 80% of accuracy.
</div>
```{r}
predicted.prob <- predict( training_model, newdata=test, type = "response")
predicted.classes <- as.factor( ifelse( predicted.prob > 0.5, "Alive", "Died" ) )
predicted.classes <- fct_relevel( (predicted.classes) , c("Died","Alive" ) )
library(forcats)
tab_xtab(test$mort_5yr, predicted.classes, var.labels = c("Actual Life status", "Predicted life status"), show.row.prc = TRUE)
```

### Receiver Operating Characteristic (ROC)
<div class = "blue">
This ROC representation basically helps to determine the quality of models. A model having an AUC (area under the curve) very close to 1 is the best and 1, or 100%, is perfect. Here it is 0.74 which is much better than guessing, but not a reliable prediction.
</div>
```{r}
library(pROC)
test_model <- glm(mort_5yr ~ extent.factor + age.factor + node4.factor, family= binomial(logit), data = test)
invisible(plot(roc(factor(ifelse(test$mort_5yr == "Died", 2, 1)), fitted(test_model)), print.thres = c(.1, .5), col = "red", print.auc = T))
```

### Checking The Residuals. 

```{r}
actual <- test$mort_5yr  # actual  
actual.num <- ifelse( test$mort_5yr == "Alive", 1, 0)
frq( actual.num , out="v", title="Actual mort_5yr as 0 and 1 for test sample" )
```


```{r}
residuals.num <- actual.num - predicted.prob
res.act.pred <- as.data.frame(cbind(actual.num, predicted.prob, residuals.num, test[c(22, 26, 24, 30)]) )
library(kableExtra)
head(res.act.pred, 20) %>%
  kable("html", align = 'clc', digits=5, col.names=c( "actual", "predicted", "residuals", "node4", "extent", "age group", "mort_5yr")) %>%
    kable_styling(bootstrap_options = "striped", full_width = F, position = "left") %>% 
    footnote(general = "The actual and predicted values of mort_5yr and the residuals.")
```
<div class = "blue">
The following plot shows the predicted probabilities on the horizontal axis and the residuals on the vertical axis. Since the predictions naturally gravitate towards 0 and 1 as the actual values, there are two lines in the plot. The important point is that the distance between the two lines stays fairly consistent from left to right.
</div>
```{r}
plot_scatter( res.act.pred, predicted.prob, residuals.num )
```

## Findings
<div class = "blue">
The extent of the spread, the age group and the number of nymph nodes are the main predictor of a patient life status at 5 years.  **The treatment is not a predictor.**

Even so, the model can not be used reliably to determine the 5 year life status of a patient; its hit-ratio doesn't exceed 67%.
</div>

# Stepwise Regression on Time Until Death in Days
## Multiple Regression Using All Predictor Variables
<p style="border:3px; border-style:solid; border-color:#2B547E; padding: 1em;">
$H_o$: coefficient  = 0, a risk of a Type 1 error, = $\alpha = 0.05$  
$H_a$: coefficient ≠ 0  
</p>

```{r ,  fig.width=5, fig.height=5, comment=NA}
RegModel.1 <- 
  lm(time.years ~ rx + sex + age + obstruct + perfor + adhere + differ + extent+surg+loccomp+node4,
   data=colon.data)
```
```{r}
library(sjPlot)
tab_model(
 RegModel.1 ,
  title = "Regression Model of time.years (based on full sample) ",
  show.stat = TRUE,
  digits = 3,
  string.stat = "t-value",
  string.p = "p (sig)",
  show.fstat = TRUE,
  show.dev = TRUE,
  show.aic = TRUE,
  CSS = list(
    css.depvarhead = 'color: red;',
    css.centeralign = 'text-align: left;',
    css.firsttablecol = 'font-weight: bold;',
    css.summary = 'color: blue;'
  )
)
```


```{r plotreg, message=FALSE, echo=TRUE, fig.width=4, fig.height=5}
plotreg(RegModel.1, custom.model.names ="Regression of time.years")
```

## Reducing the model to those variables able to predict well

There are two primary ways to reduce the size of a predictive model based on statistical analysis: 

__1__ Delete those variables that have p-values greater than 0.05; 

__2__ Delete those variables having dangerously high multicollinearity.


## Investigation of Multicollinearity within Predictor Variables

<div class = "blue">
Multicollinearity is the occurrence of high intercorrelations among two or more independent variables in a multiple regression model.
Multicollinearity in a multiple regression model indicates that collinear independent variables are related in some fashion, although the relationship may or may not be casual.

Before beginning the regression, the correlations below may provide some insights.
__The correlation coefficient__ indicates the strength of the linear relationship that might be existing between two variables.
</div>

```{r fig.width=9, fig.height=9}
rr <- colon.data[c(4, 6,7,8,9,10,11,12,13,14,15,29,31,32)]
library(corrplot)
M <- cor(rr) 
corrplot.mixed(M, upper = "ellipse",lower='number')
```

## Relative Importance of Predictors (Drivers)

The table below presents all of the relative importance metrics from ‘relaimpo’. 
 
The second block gives the coefficients when 1 predictor is entered, 2 are entered through to all 11 predictors.
```{r}
RegModel.1 <- 
lm(time.years~rx + sex + age + obstruct + perfor + adhere + differ + extent+surg+loccomp+node4,
   data=colon.data)
library(car)
(vf <- vif(RegModel.1) )
```

```{r}
library(relaimpo) 
imp.lm<-calc.relimp(RegModel.1, type=c("lmg"), rela = TRUE)
imp.lm
```


```{r}
rel.lmg<- calc.relimp(RegModel.1, type="lmg")
pm<- data.frame(rel.lmg$lmg, rel.lmg$lmg.rank)
pm<-pm[order(rel.lmg$lmg.rank),]
pm <-data.frame(Features=rownames(pm), pm, row.names=NULL)
colnames(pm) <- c("Features", "Importance", "Rankings")
is.num <- sapply(pm, is.numeric)
pm[is.num] <- lapply( pm[is.num], round, 3)
tab_df(pm)
```


The tables above answer the question of 

__What are the key drivers of mort_5yr:	5 years life status ?__


## Plot: Key Drivers of mort_5yr:	5 years life status

```{r}
theme_set(  theme_bw( ))
ggplot(pm, aes(x= reorder( Features,  Importance ), y=  Importance ) ) +
  geom_bar(stat="identity", fill="lightblue", colour="black") + 
  coord_flip() + 
  ggtitle("Key Drivers of time.years") +
  geom_text(aes(label=format(Importance, digits=2), size=0.5, hjust=1))+
  theme(legend.position="none") +
  theme(axis.text.y=element_text(face="bold", color="black",size=12))+
  theme(plot.title=element_text(
                                face="bold", color="black",size=18))+
  theme(axis.title.x=element_text(
                                  face="bold", color="black",size=14))+
  theme(axis.title.y=element_text(
                                 face="bold", color="black",size=14))+
  labs(y= "Relative impacts of Variables", x= "Variables")
```


## Reducing the model to significant predictors


# Stepwise Regression

```{r}
library(MASS)  
step <- stepAIC(RegModel.1, direction="both")
step
```


The stepwise regression (or stepwise selection) consists of iteratively adding and removing predictors, in the predictive model, in order to find the subset of variables in the data set resulting in the best performing model, that is a model that lowers prediction error.

__Stepwise selection (or sequential replacement), which is a combination of forward and backward selections___. It starts with no predictors, then sequentially adds the most contributive predictors (like forward selection). After adding each new variable, any variables that no longer provide an improvement in the model fit are removed. (like backward selection). 
It stops when a model is obtained where all predictors are statistically significant.

The AIC at the top of each regression is the ___Akaike Information Criterion___. It compares the quality of a set of statistical models to each other. We only compare AIC value whether it is increasing or decreasing by adding more variables. Also in case of multiple models, the one which has lower AIC value is preferred.

However, AIC will choose the best model from a set, it won’t say anything about absolute quality.Therefore, once the best model has been selected, a hypothesis test will be run to figure out the relationship between the variables in the model selected and mort_5yr: 5 years life status


<div class = "blue">

However, AIC will choose the best model from a set, it won’t say anything about absolute quality.Therefore, once the best model has been selected, __a hypothesis test__ will be run to figure out the relationship between the variables in the model selected and LifeExpectancy.

</div>



__Taking the coefficients from the so-called best model from the stepwise process and using them as the final model__

## Stating and testing the Hypothesis for the variables (rx.factor + age.factor + differ.factor + extent.factor) in the model selected (best model)

__$H_o$__: the coefficients of the variables in the the best model are equal to zero, statistically, at a risk of a Type 1 error, = $\alpha = 0.05$

__$H_a$__: the coefficients of the variables in the the best model are NOT equal to zero, statistically, at a risk of a Type 1 error, = $\alpha = 0.05$

```{r}
tab_model(step,
  title = "Regression of time.years on 'best' stepwise model",
  show.stat = TRUE,
  digits = 3,
  string.stat = "t-value",
  string.p = "p (sig)",
  show.fstat = TRUE,
  show.dev = TRUE,
  show.aic = TRUE,
  CSS = list(
    css.depvarhead = 'color: darkgreen;',
    css.centeralign = 'text-align: left;',
    css.firsttablecol = 'font-weight: bold;',
    css.summary = 'color: blue;'
  ))
```

## Plotting the model coefficients (the best model with select 4 variables and the original model with all 8 predictor variables)

```{r ,  fig.width=5, fig.height=5, comment=NA}
RegModel.2 <- 
  lm(time.years~ rx + age + differ + extent + loccomp + node4,
   data=colon.data)
```
```{r mlr,  fig.width=9, fig.height=9, comment=NA}
plot_models(
  RegModel.1,RegModel.2 ,
  title = "Regression models with 4 predictor variables and with 8 predictor variables",
  show.values = TRUE,
  show.intercept = TRUE,
  show.p = TRUE,
  m.labels = c( "original model", "best model"),
  p.shape = TRUE,
  grid = FALSE,
  vline.color = "gray",
  ci.lvl = 0.95
) 
```


## The model reduced to only variables having coefficients with significant differences from zero.

```{r ,  fig.width=5, fig.height=5, comment=NA}
RegModel.2 <- 
  lm(time.years ~ rx + age + differ + extent + loccomp + node4,
   data=colon.data)
tab_model(
  RegModel.1,RegModel.2 ,
  title = "Regression models with 4 predictor variables and with 8 predictor variables",
  show.stat = TRUE,
  digits = 3,
  string.stat = "t-value",
  string.p = "p (sig)",
  show.fstat = TRUE,
  show.dev = TRUE,
  show.aic = TRUE,
  linebreak = TRUE,
  CSS = list(
    css.depvarhead = 'color: darkorange;',
    css.centeralign = 'text-align: left;',
    css.firsttablecol = 'font-weight: bold;',
    css.summary = 'color: blue;'
  )
) 
```



The RegModel.2 model above (model to the right), obtained through the stepwise regression (or stepwise selection) has the predictor variables with non-significant p-values (<0.05) that can effectively explain the response variable . 
.

Hence __RegModel.2 model__ above (model to the right) and shown in the graph below will be our __‘final’ model with predictor variables rx.factor + age.factor + differ.factor + extent.factor__.


<br><br> This graph shows those variables whose coefficients are significantly (p<0.05) __different from zero___ in __red__ and those __not significantly different from zero__ in __blue__.

This graph shows all variables in red, that have coefficients, significantly (p<0.05), different from zero. Since variables with coefficients not significantly different from zero have not been included hence there are no blue dots in the graph. 


```{r , message=FALSE, echo=TRUE, fig.width=4, fig.height=5}
plotreg(RegModel.2, custom.model.names ="Regression models of time.years with significant predictor variables")
```


## Relative importance of the reduced model on all observations

```{r}
library(relaimpo) 
imp.model.2<-calc.relimp(RegModel.2,type=c("lmg"),rela = TRUE)
imp.model.2
```

## Relative Importance of Predictors (Drivers) in the new/final model

```{r}
rel.lmg<- calc.relimp(RegModel.2, type="lmg", rela=TRUE)
pm<- data.frame(rel.lmg$lmg, rel.lmg$lmg.rank)
pm<-pm[order(rel.lmg$lmg.rank),]
pm <-data.frame(Features=rownames(pm), pm, row.names=NULL)
colnames(pm) <- c("Features", "Importance", "Rankings")
is.num <- sapply(pm, is.numeric)
pm[is.num] <- lapply( pm[is.num], round, 3)
tab_df(pm)
```
```{r}
theme_set(  theme_bw( ))
ggplot(pm, aes(x= reorder( Features,  Importance ), y=  Importance ) ) +
  geom_bar(stat="identity", fill="darkorange1", colour="black") + 
  coord_flip() + 
  ggtitle("Key Drivers of 'time.years (best model)") +
  geom_text(aes(label=format(Importance, digits=2), size=0.5, hjust=1))+
  theme(legend.position="none") +
  theme(axis.text.y=element_text(face="bold", color="black",size=12))+
  theme(plot.title=element_text(
                                face="bold", color="black",size=18))+
  theme(axis.title.x=element_text(
                                  face="bold", color="black",size=14))+
  theme(axis.title.y=element_text(
                                 face="bold", color="black",size=14))+
  labs(y= "Relative impacts of Variables", x= "Variables")
```


## Multicollinearity in the reduced model

Variance inflation factor (VIF): The measure of multicollinearity among the independent variables in a multiple regression model.
__Table:Variance inflation factor (VIF)__ 

```{r}
library(car)
(vf <- vif(RegModel.2) )
```

The smallest possible value of VIF is 1 (absence of multicollinearity). As a rule of thumb, a VIF value that exceeds 4 indicates a problematic amount of collinearity amongst the variables.

## Building a regression model using the training sample data. Reducing the model to those variables having coefficients significantly different from zero. Explaining the process and the final model.

The model will attempt to learn the relationship on the training data and be evaluated on the test data.
Cross-validation is primarily used in to estimate the skill of a model on unseen data. That is, to use a limited sample in order to estimate how the model is expected to perform in general when used to make predictions on data not used during the training of the model.
Overfitting and underfitting is a fundamental problem that trips up even experienced data analysts. The developed model may look great, but the problem is if a testing set is never used, the model is nothing more than an overfit representation of the training data. Thus the model does not work when someone else tries to apply it to a new data.

___How good is the model?___

I’ve built a model, investigated statistical properties, considered the relative importance of the predictors and reduced the model to those most important predictors that are significantly related to the response variable. Now, we need to further investigate model quality and usability.Quality of regression models can be judged in many ways. In this case it will be done by splitting the dataset randomly into a training sample (70% of total observations) on which the model will be re-developed and a testing sample, sometimes called the holdout sample, on which the quality of predictions is tested.

## Building a regression model using the training sample data.


```{r }
model.tr<-lm(time.years~ rx + age + differ + extent + loccomp +  node4, train)
# summary(model.tr) # remove the hash-tag at the beginning of this line to see the standard summary
tab_model(
  model.tr,
  title = "Regression models of time.years using the Training Sample",
  show.stat = TRUE,
  digits = 3,
  string.stat = "t-value",
  string.p = "p (sig)",
  show.fstat = TRUE,
  show.dev = TRUE,
  show.aic = TRUE,
  CSS = list(
    css.depvarhead = 'color: red;',
    css.centeralign = 'text-align: left;',
    css.firsttablecol = 'font-weight: bold;',
    css.summary = 'color: blue;'
  )
)
```


### Stating and testing the Hypothesis for the variables in the __"training_model"__

__$H_o$__: the coefficients of the variables in the the training_model are equal to zero, statistically, at a risk of a Type 1 error, = $\alpha = 0.05$

__$H_a$__: the coefficients of the variables in the the training_model are NOT equal to zero, statistically, at a risk of a Type 1 error, = $\alpha = 0.05$
 

### Reducing the model, using Stepwise Regression, to those variables having coefficients significantly different from zero

Since the variable 'Rural' has coefficient that __is  significantly not different from zero (p (sig) = 0.058 > 0.05)__. Hence __$H_o$__ cannot be rejected in this case. 

The stepwise regression was used to find the "best training model" with subset of predictor variables that are statistically significant.

```{r}
library(MASS)  
step <- stepAIC(model.tr, direction="both", trace = TRUE)
step
```
In case of current stepwise regression for LifeExpectancy using the Training Sample, the model with variables 'BirthRate', 'GDP' , 'Health' , 'HIV' , 'Rural' has been chosen as the best training model. 
The “plotreg()” function is used below to present the coefficient estimates based on the training sample. The graphs show that, while the intercept is not significantly different from zero, all the five predictor variables significantly help to improve prediction of LifeExpectancy. 
```{r}
plotreg(model.tr, custom.model.names =" Regression models of time.years using the Training Sample")
```

## Reporting on the relative importance of those variables that remain in the model with a table, graph and in words. 

```{r}
library(relaimpo) 
imp.model.3<-calc.relimp(model.tr,type=c("lmg"),rela = TRUE)
imp.model.3
```

```{r}
rel.lmg<- calc.relimp(model.tr, type="lmg", rela=TRUE)
pm<- data.frame(rel.lmg$lmg, rel.lmg$lmg.rank)
pm<-pm[order(rel.lmg$lmg.rank),]
pm <-data.frame(Features=rownames(pm), pm, row.names=NULL)
colnames(pm) <- c("Features", "Importance", "Rankings")
is.num <- sapply(pm, is.numeric)
pm[is.num] <- lapply( pm[is.num], round, 3)
tab_df(pm)
```


___lmg calculates the relative contribution of each predictor to the R square with the consideration of the sequence of predictors appearing in the model___.

Using the 'lmg' metric, the tables above answer the question of __What are the key drivers of mort_5yr:5 years life status in the training model__.

___Plot: Key Drivers of time.years___

The key drivers of variable time.years can be visually represented using the following diagram. 

```{r}
theme_set(  theme_bw( ))
ggplot(pm, aes(x= reorder( Features,  Importance ), y=  Importance ) ) +
  geom_bar(stat="identity", fill="lightgreen", colour="black") + 
  coord_flip() + 
  ggtitle("Key Drivers of time.yearsin Training Model") +
  geom_text(aes(label=format(Importance, digits=2), size=0.5, hjust=1))+
  theme(legend.position="none") +
  theme(axis.text.y=element_text(face="bold", color="black",size=12))+
  theme(plot.title=element_text(
                                face="bold", color="black",size=18))+
  theme(axis.title.x=element_text(
                                  face="bold", color="black",size=14))+
  theme(axis.title.y=element_text(
                                 face="bold", color="black",size=14))+
  labs(y= "Relative impacts of Variables", x= "Variables")
```


## Reporting on the residuals and test if the distribution of the residuals is normal. Why is this important? 

## The residuals
```{r}
predicted <- predict(model.tr, newdata=test  )
actual <- test$time.years # actual Q3a attitudes for testing sample
residual <- predicted - actual   
x<- as.data.frame(cbind(actual, predicted, residual)) 
```
```{r}
library(kableExtra)
head(x, 10) %>%
  kable("html", align = 'clc', digits=2, col.names=c( "actual", "predicted", "residuals")) %>%
    kable_styling(bootstrap_options = "striped", full_width = F, position = "left") %>% 
    footnote(general = "The actual and predicted values of time.years and the residuals for the first 6 Patients.")
```

## Testing if the distribution of the residuals is normal

__Stating Null and Alternative Hypothesis:__ 

$H_o$: Distribution of residuals is Normal , risk of a Type 1 error = $\alpha = 0.05$.   

$H_a:$ Distribution of residuals is not Normal

```{r  fig.width=5, fig.height=5 }
library("car")
qqPlot(model.tr, main="QQ Plot") 
```


The plot above shows the distribution of the residuals against the expected normal distribution. For normally distributed data, observations should lie approximately on the reference line. Since, there are some (approximately 4) outliers in the data (points at the ends of the line distanced from the bulk of the observations and the reference line) the residuals may not be normally distributed. Hence, further investigation is required. 

*******

In the density plots below, we attempt to visualize the underlying probability distribution of the residuals by drawing an appropriate continuous curve. 
Residuals do not seem to be normally distributed as residuals are not symmetric about the mean and show bimodality. It should be further investigated by the conducting Normality testing.

```{r fig.height=5, fig.width=5, message=FALSE, warning=FALSE, comment=NA, include=FALSE}
d <- densityPlot(residual)
```
```{r echo=TRUE, message=FALSE, warning=FALSE, fig.width=5, fig.height=5, comment=NA}
plot(d, main=" Density Plot Residuals")
polygon(d, col="orange", border="green")
```

__Testing the hypothesis using Shapiro-Wilk’s method__

Shapiro-Wilk’s method has been used to test the normality of Residuals, considering a significance level of 0.05. 

```{r echo=TRUE, message=FALSE, warning=FALSE, fig.width=5, fig.height=5, comment=NA}
shapiro.test((residual))
```



## How Important Are Normal Residuals in Regression Analysis?

Prediction intervals are calculated based on the assumption that the residuals are normally distributed. If the residuals are non-normal, the prediction intervals may be inaccurate, affecting the reliability and usability of the model.
Moreover, while building a optimally fitting linear model, it is assumed that the relationship is linear, and that the errors, or residuals, are simply random fluctuations around the true line. If residuals are not (approximately) normally distributed (with a mean of zero), it can pinpoint potential issues with the model.

## 2.e. Testing homoscedasticity and Stating hypotheses. 

The assumption of homoscedasticity (meaning “same variance”) is central to linear regression models.  Homoscedasticity describes a situation in which the error term (that is, the “noise” or random disturbance in the relationship between the independent variables and the dependent variable) is the same across all values of the independent variables. 

Homoscedasticity in the best model  is tested below using the Breusch Pagan Test. It tests whether variances of residuals from a regression are dependent on the values of an independent variable.

__Stating Null and Alternative Hypothesis:__ 

$H_o:$ there are equal or constant variances,  $\alpha$ = 0.05

$H_a:$ there are unequal or non-constant variances    

```{r}
pacman::p_load( "olsrr" )
ols_test_breusch_pagan(RegModel.2)
```



## Using the model based on the training sample to predict mort_5yr:5 years life status for those in the testing sample. How good is the model based on the training sample when used on the testing sample data? 

## Predicting mort_5yr:5 years life status using the training model on the testing dataset

“predict()” evaluates the model equation on values of rx.factor + age.factor + differ.factor + extent.factor for each respondent in the testing sample. While the model was based on those respondents in the training sample, the respondents in the testing sample did not contribute to those calculations and can be considered to be “fresh” or “new” subjects.

__The code-chunk below predicts the mort_5yr:5 years life status for the test, or holdout, sample of patients using the model built on the training data.__

```{r}
model.te<-lm(time.years ~ rx + age + differ + extent + loccomp + node4, test)
predicted.tr.te <- predict(model.tr, newdata=test  )
predicted.tr.te
actual <- test$ time.years # actual attitudes for the testing sample
x<- as.data.frame(cbind(actual, predicted.tr.te)) 
```

## Assessing the model based on the training sample when used on the testing sample data. 

### Scatter plots of Predicted vs Actual

```{r,  fig.width=5, fig.height=5}
scatterplot(x$predicted~x$actual, regLine=TRUE, smooth=FALSE, boxplots=FALSE, data=colon.data,col = "red", main = "Scatterplot of Predicted vs Actual " ,xlab = "Actual",ylab = "Predicted")
```

Scatter plots of Predicted vs Actual is used to understand how well the regression model makes predictions for different response values.  A perfect regression model has a predicted response equal to the true response, so all the points lie on a diagonal line. The vertical distance from the line to any point is the error of the prediction for that point. A good model has small errors, and so the predictions are scattered near the line.

Hence, it may be concluded that the model has a weak Goodness of fit since most of the points are foggy or dispersed (away from the diagonal line). 

The analysis above, further mandates the assessment of goodness of the model based on the training sample when used on the testing sample data. 

### Correlation between actual and predicted values of the dependent variable, Life Expectancy

___Stating the Hypothesis___

Ho: correlation = 0 i.e., actual and predicted values of the dependent variable, Life Expectancy, are __statistically independent__, a risk of a Type 1 error, = α=0.05

Ha: correlation ≠ 0 i.e., actual and predicted values of the dependent variable, Life Expectancy, are __statistically correlated__ , a risk of a Type 1 error, = α=0.05 are correlated a risk of a Type 1 error, = α=0.05


___Testing Hypothesis___


```{r,  fig.width=5, fig.height=5}
( c<- cor.test(x$actual,x$predicted))
library(sjPlot)
tab_corr(x, digits=2, show.p=TRUE, p.numeric=TRUE)
```

___Interpretations___

From the analysis above, it can be inferred that actual and predicted values of the dependent variable, Life Expectancy, are significantly correlated (r = 0.9052694 , p = 0.00000009964). Since p = 0.00000009964 < 0.05, the null hypotheses can be rejected. Also, the confidence interval does not include 0.

### Global test of model assumptions using the ‘gvlma’ package

The ‘gvlma’ package provides a summary assessment of the assumptions for a linear model and they are below for the testing model.
```{r,  fig.width=5, fig.height=5}
pacman::p_load(gvlma)
gvmodel <- gvlma(model.te) 
summary(gvmodel)
```



___Interpretations___

The summary above provides the following information: 

The intercept with a value of 76.57593133 is well above the origin i.e., the regression line does not pass through the origin.

GDP & Health variables have positive coefficients which indicate direct impact on the dependent variable, LifeExpectancy, whereas HIV, Rural and BirthRate have negative coefficients which indicate inverse relationship with the dependent variable, LifeExpectancy. 

The t-statistic/t-value is just the estimated coefficient divided by its own standard error. Thus, it measures “how many standard deviations from zero” the estimated coefficient is.

From the p value of 0.000002631 we can estimate that statistically intercept goes beyond zero and can reject the null hypotheses that the line intercepts the vertical axis at the origin, α = 0.05.



# Survival analysis
Just like with the regression models, we have to start with a null model.  The table for KM analysis on all cases for the null model is below.

```{r }
surv.colon <- survfit(Surv(time, status) ~ 1,  data = colon.data)
summary(surv.colon)
```

This can be displayed visually with the corresponding graph. 

```{r }
plot(surv.colon,  xlab="Days till death", ylab="Survival Probability")
title("Survival curves for colon data, Null model")
```

```{r }
library(survminer)
ggsurvplot(surv.colon, data = colon.data, pval = TRUE)
```
<div class = "blue">
Since RX treatment has not been entered into the model above, the table and graph above convey that the probabilities of survival from colon cancer at each point in time (days). the lower the days, the higher probability of survival. at 23 days time, there are 614 people at risk with a 98% chance of survival. However as the days increase, the chances of survival go down. We see this illustrated in both the graph and table. For example, at 797 days time, there are 469 peoples with a chance of survival below 76%.
</div>
## Survival Model for the RX Treatment on All Cases
First this is done by showing the model for each treatment type.
```{r }
surv.colon <- survfit(Surv(time,status) ~ rx, conf.int=TRUE, data = colon.data)
summary(surv.colon)
```
Then displayed graphically
```{r }
plot(surv.colon, lty = c(2,1), xlab="Days to next event", ylab="Survival Probability", col=c("red","green"))
legend("topright", legend=c("Probability of survival: Standard Treatment", "Probability of survival: Test Treatment"),fill=c("red","green"),bty="n")
title("Survival curves for Lung Cancer data")
```

```{r }
ggsurvplot(surv.colon, data = colon.data, pval = TRUE, conf.int=TRUE)
```
<div class = "blue">
Above we see the survival analysis divided into two groups. Group one being the Lev treatment and group 2 being the Lev+5FU treatment. Some observations include:

For the Lev treatment group, in 93 days time, 308 people had survived and were at risk with a 99% chance of survival. 

For the Lev+5FU treatment group, in 79 days time, 300 people had survived and were at risk with a 98% chance of survival.
</div>
## Test statistically if the two treatments are the same or different.

Let's define our hypotheses
<p style="border:3px; border-style:solid; border-color:#2B547E; padding: 1em;">
$H_o:$ There's no difference between the Lev treatment and Lev+5FU treatment, $\alpha = 0.05$  
$H_a:$ There is a difference 
</p>

A statistical analysis can be done to prove the differences within the treatment.
```{r }
survdiff(Surv(time, status)~ rx, data=colon.data, rho=0)
```
<div class = "blue">
Above we see a chi square value of 8.2 and a p value of .004. We do reject the null hypothesis as the p-value is .004 and well below our threshold of .05. This means there is a significant difference between the Lev treatment and Lev+5FU colon cancer patients.
</div>
# Summative Conclusion
## Distributions of Variables
## Correlation of Variables
## Stepwise (Linear) Regression
## Logistic Regression
## Survival Analysis
# Application

##ggsurvplot##


```{r}
ggsurvplot(surv.colon, data = colon.data, pval = TRUE, conf.int = TRUE, risk.table = TRUE,  surv.median.line = "hv", ggtheme = theme_bw(),  palette = c("#E7B800", "#2E9FDF"), ncensor.plot = TRUE, break.time.by = 300)
```
<div class = "blue">
In the plot above:

* The horizontal axis (x-axis) represents time in days, and the vertical axis (y-axis) shows the probability of surviving or the proportion of people surviving. The lines represent survival curves of the two treatment groups. A vertical drop in the curves indicates an event. The vertical tick mark on the curves means that a patient was censored at this time.
* Confidence bands for each survival curves are displayed in their respective colors.
* Confidence bands are wide at the tail.
* At time zero, the survival probability is 1.0 (or 100% of the participants are alive).
* From the time of approx 10 to 100, probability of survival for those on standard treatment are higher than probability of those on test treatment.
* After the time approx 100, the probability of survival for those on the standard treatment gets lower compared to the probability of survival for those on the test treatment.
* Last person on Standard treatment is around the time 600 whereas test treatment last person is at time 1000.
* From the graph, it shows the test treatment is less effective than standard treatment in the beginning phase (till time 100) and after that test treatment is superior than the standard treatment.
* p-value is the p-value of the Log-Rank test comparing the treatment groups
* Risk table below the plot shows absolute number of individuals at risk for both the treatment groups.
* n.censor plot displays the number of censor subjects at the time. There is no censor after time approx 250.
</div>
