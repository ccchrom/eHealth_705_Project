---
title: "eH705 Final Project"
subtitle: "Chemotherapy for Stage B/C Colon Cancer"
author:
  name: Neha Kalra, Matthew Jabora, Ibrahim Zifa, Sandeep Delar, Romuald Lamps 
  affiliation: eH705 | eHealth MsC
date: "`r format(Sys.time(), '%d %B %Y')`"

output:
  html_document: 
    number_sections: TRUE
    code_folding: hide
    toc: true
    toc_float: true
    toc_depth: 4
    toc_collapsed: true
    theme: readable
--- 

```{r setup, eval=TRUE, include=FALSE}  
knitr::opts_chunk$set( echo=TRUE, warning=FALSE, message=FALSE, error=FALSE, comment = NA )
options(max.print = 10000)
options(scipen = 999)
pacman::p_load(DT, kableExtra, magrittr, ggplot2, wrapr, generics, car, psych, xray, texreg, dplyr, wrapr,  sjPlot, knitr, DescTools, Hmisc, corrplot, ggcorrplot, qgraph, corrr, tidyverse, sjmisc, sjlabelled, sjstats, forcats, kableExtra, captioner, unikn, captioner, hrbrthemes, relaimpo, plotflow, tidyverse, dials, tidymodels, ggeffects, haven, glmmTMB, broom, magick, BiocManager, vtree, pscl, qgraph, unikn, car, MASS, hrbrthemes, plotflow)
```   
```{r "colorize", echo=FALSE, results="hide"}
colorize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color, 
      x)
  } else x
}
```  

# Objectives 

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">
We wish to confirm using our own process that adjuvant therapy with the combination of levamisole and fluorouracil significantly reduce death within 5 years compared with no adjuvant therapy – using only levamisole.  We chose to only include these two groups because we are only comparing adjuvant therapy against normal therapy.  Levimasole is already recognized worldwide as an antiparasitic drug that changes ones immune response, similar to methotrexate – so we are considering it a control.  We aim to achieve this by:<br>

1.  Assessing the distributions of variables within the two treatment categories, including: (distribution plots/tables between groups)  
    a.	Sex – Male or female patients  
    b.	Age – Finding the average (or median) age that has most impact on 5 year survival  
    c.	Obstructive tumours versus non-obstructive  
    d.	If the colon was perforated by the tumor or not  
    e.	If tumors were adhered to nearby organs  
    f.	Differentiation of tumors – rated well, moderate, or poor  
    g.	The extent of local spread of the tumor which could be:  
        i.	Submucosal  
        ii.	Within muscle  
        iii.	Within the serosa – the outermost layer of organs  
        iv.	Contiguous structures – meaning nearby organs  
2.  Investigating if any of the predictor variables have an impact on each other. (correlation)(VIF)  
3.  Using stepwise (linear) regression to obtain a model that best describes the impact of specific variables on time until death.  
4.	Using a logistic regression model to investigate the association between five year life status and one or more categorical predictor variables within each treatment group (regression models for both groups) (predict) (train+test) (residuals)  
5.	Assessing whether there are differences in survival (or cumulative incidence of event) among different groups of participants  
    a.  This will be done by testing the null hypothesis of no difference in survival between two or more independent groups.  
        i.	H0: The two survival rates are identical, (α=0.05).    
        ii.	Ha: The two survival rates are not identical.  (chi-square test)  
6.	Analyzing data within the subset of Levamisole-fluorouracil to find which variables have the greatest impact on 5 year mortality.  (Reliampo)  
</div>

# Read the file  
<div class = "blue">
This data comes from a study investigating the addition of fluorouracil to the treatment regime of Levamisole in subjects with colon cancer.<br>
Moertel, C., Fleming, T.,Macdonald, J., Haller, D., Laurie, J., Goodman, P., Ungerleider, J., Emerson, W., Tormey, D., Glick, J., Veeder, M. & Mailliard, J. (1990). Levamisole and Fluorouracil for Adjuvant Therapy of Resected Colon Carcinoma. <i> The New England Journal of Medicine. </i> 332(<i>6</i>).
</div>

```{r}
colon.data <- read.csv('colon_s.csv')
str(colon.data)
```

# Investigating, Cleaning and Recoding the Data  
## Column descriptions  

The description of the variables for this dataset are listed below.

```{r}
df = as.data.frame(matrix( 
  c('id','id',
    'rx','Treatment - Obs(ervation), Lev(amisole), Lev(amisole)+5-FU',
    'sex','sex of the patient (1 = Male, 0 = Female)',
    'age','age in years',
    'obstruct','obstruction of colon by tumour',
    'perfor','perforation of colon',
    'adhere','adherence to nearby organs',
    'nodes','number of lymph nodes with detectable cancer',
    'status','censoring status',
    'differ','histological differentiation of tumour (1=well, 2=moderate, 3=poor)',
    'extent','Extent of local spread (1=submucosa, 2=muscle, 3=serosa,
4=contiguous structures)',
    'surg','time from surgery to registration (0=short (7-20 days), 1=long (21-35 days))',
    'node4','more than 4 positive lymph nodes',
    'time','days until death',
    'loccomp','location of primary tumor - mi;to[;e ',
    'time.years','years until death',
    'mort_5yr','5 years life status',
    'age.10','Age divided by 10..',
    'hospital','Hospital'), # the data elements 
  ncol = 2,              # number of rows 
  byrow = TRUE))
colnames(df) <- c("Variable", "Description")
df %>%
  kable(caption = "World Population Health: All Countries") %>%
  kable_styling( bootstrap_options = "striped", full_width = F, position = "left", fixed_thead = T)
```

## Overview of Data
<div class = "blue">
Below tree diagram shows:

* Total number of observations- 929
* 33% of the observations are for patients who were given levamisole (Lev).
* 33% were given levamisole + fluorouracil (Lev +5FU)
* 34% constitutes the controls group. *Note: the control group data will be deleted below*
* There is almost equal distribution of male and females in both the groups (Lev and Lev+5FU)
* Majority of the patients belong to the age group of 60+ followed by 40-59 and <40 years.
</div>

```{r echo=FALSE, , fig.align="center", fig.width=6, fig.height=6, fig.cap="Figure 1: Tree Diagram of Colon Data by Rx, Sex and Age Group"}
library(vtree)
vtree(colon.data, c("rx.factor", "sex.factor", "age.factor"), horiz = FALSE, fillcolor = c(restecg = "#e7d4e8", cp = "#99d8c9"), keep = list(restecg = c("ST-T abnormality")))
```
## Converting Variables 
<div class = "blue">
Below, character variables are converted to factors, integers are converted to numbers, and the observation arm is filtered out.  This is done to assist with our analysis.  The removal of the observation group done to primarily focus on the comparison between adjuvant therapy and the current standard at that time, and to also focus on other factors - aside from treatment - that may effect survival within that group.
</div>
```{r}
library(dplyr)
colon.data <- colon.data %>% filter(rx!="Obs")
colon.data <- colon.data %>% mutate_if(is.character, as.factor)
colon.data <- colon.data %>% mutate_if(is.integer, as.numeric)
colon.data$mort_5yr <- fct_relevel( (colon.data$mort_5yr) , c("Died", "Alive" ) )
library(Hmisc)
describe(colon.data)
```
## Missing Values and Treatment
In this section, the total count of missing values (NA) in each column is identified and treated *Note* - Zeros are not treated as part of treating missing values as zeros are one of the valid response outputs.
### Total Count of Missing Values>
<div class = "blue">
The summary table below provides total count of NA for all the variables along with other information like minimum value, Max.value, Mean, Median etc. From the table below it is identified 6 of the variables contain NAs that need to be treated for further analysis.
</div>
```{r}
MD <- colSums(is.na(colon.data))
library(kableExtra)
MD[c(3:14,27,29:30)] %>%
  kable(caption = "Table 3: Total count of NA for each variable") %>%
  kable_styling( bootstrap_options = "striped", full_width = F, position = "left", fixed_thead = T)
```
<div class = "blue">
  
It is noted that within this dataset, that there are already factor versions of the variables.  To be sure not to double count, below is a summary of the NAs:
* Below are the columns identified with NA:
   * obstruct - 15
   * nodes - 15
   * differ - 16
   * surg - 11
   * loccomp - 14
   * mort_5yr - 8
   
The total of NAs is 79 - which is roughly 12% of the filtered sample of 614.  We chose to omit these values due to the large sample size.  There was worry that if we imputed this data - which is mostly two-level factors - that there may be a large margin of error and it may produce bias in our model.
</div>
```{r}
library(dplyr)
colon.data <- colon.data %>%
filter(rx!="Obs") %>%
na.omit()
```
# Investigating the Cleaned Data
## Visual Representation 
The below table and vtree give a quick glance at the cleaned data
```{r}
colon.data %>% datatable()
```
<div class = "blue">
 
Below tree diagram shows:
* Total number of observations in the Cleaned Dataset- 551
* 51% of the observations are for patients who were given levamisole (Lev).
* 49% were given levamisole + fluorouracil (Lev +5FU)
* There are almost equal distribution of males and females in the study. There are more females on the Lev+FU treatment.
* Majority of the study population is from +60 age group followed by 40-59 years and a very small propoortion belong to <40 years age group. There is approximately equal distribution of the population in the 3 age group categories for both the treatments.
</div>
```{r echo=FALSE, , fig.align="center", fig.width=6, fig.height=6, fig.cap="Figure 2: Tree Diagram of Cleaned Colon Dataset"}
library(vtree)
vtree(colon.data, c("rx.factor", "sex.factor", "age.factor"), horiz = FALSE, fillcolor = c(restecg = "#e7d4e8", cp = "#99d8c9"), keep = list(restecg = c("ST-T abnormality")))
```
## Preliminary Statistical Findings
<div class = "blue">
Below are the preliminary findings based on the graphs and the summary below.
The cleaned, recoded sample has total of 551 observations.
Some preliminary observations for the variables used in the analysis below:
**Demographic Distribution**
* Sex - A fairly equal distribution between Male and Female.  
* Age Group - Majority of the observation belongs to 60+ age group.
      
**Treatment Indicators**
* Treatment - 15 more observations in the Levimasole group.  
* Time to Treatment (surg) - Majority of observations had a shorter time to treatment - meaning within 7 to 20 days. 
      
**Tumor Descriptors**
* Obstruction - Majority of the observations have no obstructive tumors.  
* Perforation - Majority of the observations is "No" while a very few observations belong to "Yes".  
* Adherence - Majority of the observation is "No" for adherence to nearby organs.  
* Differ - Majority of the observations belong to the "Moderate" histological differentiation category.  
* Extent - Majority of the observation belongs to tumor growth into the serosa (lining of the intestine).
* Nodes - Majority of observations had less than 4 lymph nodes affected by the tumor.  
* Time.years - The mean of years until death is 4.6.  
* Mort_5 yr - Majority of the observations belong to "Alive".
</div>
```{r echo=TRUE, message=FALSE, error=FALSE, warning=FALSE}
summary(colon.data)
```
### Visualizing Data {-}
Below graphs are plotted to understand the Demographic distribution, treatment distribution, time to treatment and the tumor descriptors.
```{r echo=TRUE, message=FALSE, warning=FALSE, error=FALSE, fig.width=8, fig.height=8, comment=NA}
library(ggplot2)
library(dplyr)
library(scales)

plotdata12 <- colon.data%>%
  count(sex.factor)%>%
  mutate(pct = n / sum(n),
         pctlabel = paste0(round(pct*100), "%"))

p16 <- ggplot(plotdata12, 
       aes(x = reorder(sex.factor, -pct),
           y = pct)) + 
  geom_bar(stat = "identity", 
           fill = "indianred3", 
           color = "black") +
  geom_text(aes(label = pctlabel), 
            vjust = -0.25) +
  scale_y_continuous(labels = percent) +
  labs(x = "Sex", 
       y = "Percent", 
       title  = "Sex Distribution")


plotdata13 <- colon.data %>%
  count(age.factor) %>%
  mutate(pct = n / sum(n),
         pctlabel = paste0(round(pct*100), "%"))

p17 <- ggplot(plotdata13, 
       aes(x = reorder(age.factor, -pct),
           y = pct)) + 
  geom_bar(stat = "identity", 
           fill = "indianred3", 
           color = "black") +
  geom_text(aes(label = pctlabel), 
            vjust = -0.25) +
  scale_y_continuous(labels = percent) +
  labs(x = "Age Group", 
       y = "Percent", 
       title  = "Age Group Distribution")

plotdata9 <- colon.data%>%
  count(rx.factor)%>%
  mutate(pct = n / sum(n),
         pctlabel = paste0(round(pct*100), "%"))

p14 <- ggplot(plotdata9, 
       aes(x = reorder(rx.factor, -pct),
           y = pct)) + 
  geom_bar(stat = "identity", 
           fill = "indianred3", 
           color = "black") +
  geom_text(aes(label = pctlabel), 
            vjust = -0.25) +
  scale_y_continuous(labels = percent) +
  labs(x = "Treatment", 
       y = "Percent", 
       title  = "Treatment Distribution")


plotdata10 <- colon.data %>%
  count(surg.factor) %>%
  mutate(pct = n / sum(n),
         pctlabel = paste0(round(pct*100), "%"))

p15 <- ggplot(plotdata10, 
       aes(x = reorder(surg.factor, -pct),
           y = pct)) + 
  geom_bar(stat = "identity", 
           fill = "indianred3", 
           color = "black") +
  geom_text(aes(label = pctlabel), 
            vjust = -0.25) +
  scale_y_continuous(labels = percent) +
  labs(x = "Surgery Time", 
       y = "Percent", 
       title  = "Surgery Time Distribution")

plotdata <- colon.data%>%
  count(perfor.factor)%>%
  mutate(pct = n / sum(n),
         pctlabel = paste0(round(pct*100), "%"))

p01 <- ggplot(plotdata, 
       aes(x = reorder(perfor.factor, -pct),
           y = pct)) + 
  geom_bar(stat = "identity", 
           fill = "indianred3", 
           color = "black") +
  geom_text(aes(label = pctlabel), 
            vjust = -0.25) +
  scale_y_continuous(labels = percent) +
  labs(x = "Perforation", 
       y = "Percent", 
       title  = "Perforation Distribution")


plotdata1 <- colon.data %>%
  count(obstruct.factor) %>%
  mutate(pct = n / sum(n),
         pctlabel = paste0(round(pct*100), "%"))

p02 <- ggplot(plotdata1, 
       aes(x = reorder(obstruct.factor, -pct),
           y = pct)) + 
  geom_bar(stat = "identity", 
           fill = "indianred3", 
           color = "black") +
  geom_text(aes(label = pctlabel), 
            vjust = -0.25) +
  scale_y_continuous(labels = percent) +
  labs(x = "Obstruction", 
       y = "Percent", 
       title  = "Obstruction Distribution")

plotdata2 <- colon.data %>%
  count(adhere.factor) %>%
  mutate(pct = n / sum(n),
         pctlabel = paste0(round(pct*100), "%"))

p03 <- ggplot(plotdata2, 
       aes(x = reorder(adhere.factor, -pct),
           y = pct)) + 
  geom_bar(stat = "identity", 
           fill = "indianred3", 
           color = "black") +
  geom_text(aes(label = pctlabel), 
            vjust = -0.25) +
  scale_y_continuous(labels = percent) +
  labs(x = "Adherence", 
       y = "Percent", 
       title  = "Adherence Distribution")

plotdata3 <- colon.data %>%
  count(differ.factor) %>%
  mutate(pct = n / sum(n),
         pctlabel = paste0(round(pct*100), "%"))

p04 <- ggplot(plotdata3, 
       aes(x = reorder(differ.factor, -pct),
           y = pct)) + 
  geom_bar(stat = "identity", 
           fill = "indianred3", 
           color = "black") +
  geom_text(aes(label = pctlabel), 
            vjust = -0.25) +
  scale_y_continuous(labels = percent) +
  labs(x = "differentiation", 
       y = "Percent", 
       title  = "Differentiation Distribution")

plotdata4 <- colon.data %>%
  count(extent.factor) %>%
  mutate(pct = n / sum(n),
         pctlabel = paste0(round(pct*100), "%"))

p05 <- ggplot(plotdata4, 
       aes(x = reorder(extent.factor, -pct),
           y = pct)) + 
  geom_bar(stat = "identity", 
           fill = "indianred3", 
           color = "black") +
  geom_text(aes(label = pctlabel), 
            vjust = -0.25) +
  scale_y_continuous(labels = percent) +
  labs(x = "differentiation", 
       y = "Percent", 
       title  = "Extent Distribution")


plotdata5 <- colon.data %>%
  count(mort_5yr) %>%
  mutate(pct = n / sum(n),
         pctlabel = paste0(round(pct*100), "%"))

p06 <- ggplot(plotdata5, 
       aes(x = reorder(mort_5yr, -pct),
           y = pct)) + 
  geom_bar(stat = "identity", 
           fill = "indianred3", 
           color = "black") +
  geom_text(aes(label = pctlabel), 
            vjust = -0.25) +
  scale_y_continuous(labels = percent) +
  labs(x = "Mortality 5 Years", 
       y = "Percent", 
       title  = "5 Years Mortality Distribution")

plotdata6 <- colon.data %>%
  count(node4.factor) %>%
  mutate(pct = n / sum(n),
         pctlabel = paste0(round(pct*100), "%"))

p07 <- ggplot(plotdata6, 
       aes(x = reorder(node4.factor, -pct),
           y = pct)) + 
  geom_bar(stat = "identity", 
           fill = "indianred3", 
           color = "black") +
  geom_text(aes(label = pctlabel), 
            vjust = -0.25) +
  scale_y_continuous(labels = percent) +
  labs(x = "4 Nodes Affected", 
       y = "Percent", 
       title  = "4 Nodes Distribution")

p08 <- ggplot(colon.data, aes(x=time.years)) + 
  geom_density() + geom_vline(aes(xintercept=mean(time.years)),
            color="blue", linetype="dashed", size=1)+
  labs(x = "Density", 
       y = "time in years", 
       title  = "Time in Years")
            
pacman::p_load("Rmisc")
library(gridExtra)
grid.arrange(p16,p17, top = "Demographic Distribution",
            layout_matrix = matrix(c(1,2), ncol=2, byrow=TRUE)) 
grid.arrange(p14,p15, top = "Treatment",
            layout_matrix = matrix(c(1,2), ncol=2, byrow=TRUE)) 
grid.arrange(p01,p02,p03,p04, top = "Tumor Descriptors",
            layout_matrix = matrix(c(1,2,3,4), ncol=2, byrow=TRUE)) 
grid.arrange(p05,p06,p07,p08, top = "Tumor Descriptors",
            layout_matrix = matrix(c(1,2,3,4), ncol=2, byrow=TRUE)) 
``` 
# Splitting into Training and Testing Samples
<div class = "blue">
In order to do further analysis, we have to create training and testing samples  The training sample will be used to create an equation to show the impact that tumor characteristics and treatment will have on life expectancy.  The test sample is used to test that equation for it's accuracy.
```{r , "sample_splitting", fig.width=5, fig.height=5}
pacman::p_load(rsample)
set.seed(78)
train_test_split <- initial_split(colon.data)
train <- training(train_test_split)
test <- testing(train_test_split)
(samp <- dim(train_test_split))
```  
After splitting the complete sample of `r samp[3]` subjects, `r samp[1]` were selected for the training sample and `r samp[2]` for the testing (or holdout) sample.  
</div>
# Logistic Regression using Five Year Mortality  
For logistic regression, we must use a factor as the response variable.  We opted to use mort_5yr as it provides inside into whether a subject was alive or deceased at that time.  The hope here is to see which factors have the biggest impact on mortality - treatment, or which tumor descriptors?

## NULL model 
The NULL model is created in order to compare our other models to it and assess for its accuracy.  It is the starting point in which we aim to lower the deviance and AIC.  The frequency distribution of mortality at five years is provided below. Notice that 57.71% of the patients are alive after 5 years. 
```{r}
frq( colon.data$mort_5yr , out="v", title= "Sample distribution between dead and alive patients")
```
<br>
<div class = "blue">
The Null model is related to the null hypothesis. For a logistic regression model like: 
$Probability(mortalityAt5years) = f( constant + \beta_1 * X_1 + \epsilon)$, the hypotheses are:

<p style="border:3px; border-style:solid; border-color:#2B547E; padding: 1em;">
$H_o: \beta_1 = 0$, $\alpha = 0.05$  
$H_a: \beta_1 \ne 0$ 
</p>

If the null hypothesis could not be rejected, the equation would be: $Probability(mortalityAt5years) = f( constant + \epsilon )$, where epsilon represents random error.

The code-chunk below sets up the null model as "mort_5yr ~ 1" and shows its "Null deviance" of 750.68, which measures the variation among values of the response variable. That deviance becomes a basis of comparison for all other models, as well as the AIC (Akaike Information Criterion - a measurement of the accuracy of the model). If a model can't significantly beat the Null model by producing a lower deviance and AIC, it is not worth pursuing.  
</div>
```{r }
# the Null model is
null_model <- stats::glm(mort_5yr ~ 1, family=binomial(logit), data = colon.data)
summary(null_model)
```  

The odds ratio is `r coef(null_model) %>%  exp()` as can be obtained by using the following code.

```{r}
coef(null_model) %>%  exp()
```  
<div class = "blue">
The odds of "Alive after 5 years" is $\frac{318}{233}$ = `r 318/233` from the frequency table above, which is the same as the odds ratio of the model above. This really just represents informed guessing. If you knew that 57.71% of the sample will be alive after 5 years and you were asked to guess if a succession of patients are alive or dead, you would tend to guess "alive" slightly often than "died". You should attempt to adjust your guessing so that "alive" is mentioned $\frac{318}{233} - 1$ = 36.48% more often than "died".

Of course, we will need some way to test if the model using a predictor variable is any better than the Null model. We will return to the Null model later and use it as a comparison to see if other models improve on the Null.
</div>
## Use All Variables to Perform Logistic Regression   
<div class = "blue">
The variables: "extent of local spread", "age" group and "more than 4 positive lymph nodes" seem to be the main predictors of the 5 year life status. 
</div>
```{r}
#  + surg.factor + node4.factor + loccomp.factor
model_reg_all <- glm(mort_5yr ~ extent.factor + age.factor  + node4.factor + obstruct.factor + perfor.factor + adhere.factor + differ.factor + surg.factor + nodes + loccomp.factor + rx.factor + sex.factor + age, family= binomial(logit), data = colon.data)
summary(model_reg_all)
```

## Create a Reduced Model
<div class = "blue">
Using the variables shown to have an impact above - extent, age, and nodes4 - we create a new model to check its deviance and AIC.  The deviance of this model is 664, lower than the NULL deviance, and its AIC (678.64) is also lower than the NULL deviance.  This shows that by focusing on these 3 variables, and not the treatment, we are able to predict mortality more accuratly.
</div>

```{r}
model_reg_extent_age_node4 <- glm(mort_5yr ~ extent.factor + age.factor + node4.factor, family= binomial(logit), data = colon.data)
summary(model_reg_extent_age_node4)
```

### Multicollinearity
<div class = "blue">
Multicollinearity is testing to assess whether these variables have an ability to impact each other, rather than solely the response variable - mort_5yr.  When doing this in logistic regression, you test using Variance Inflation Factors (VIF).  A rule-of-thumb says that if the VIF is greater than 4, there may be a severe multicollinearity problem and when above 10 multicollinearity problems are very likely.  The VIF below are all low and are of no concern.
</div>
```{r}
pacman::p_load(car)
(vf <- vif(model_reg_extent_age_node4) )
```

## Compare the 3 models
<div class = "blue">
Adding all input as predictors will give the best deviance, but it will be overfitting - allowing for errors to effect the equation. By only including the three significant predictors (as determined by their p-value), the deviance increases slightly, but the AIC decreases. The AIC is a measure adjusted to penalize the inclusion of a variable that doesn't benefit the model. 

**So we can conclude that the extent of the spread, the age group and 'more than 4 positive lymph nodes' are the main predictors of the life status at 5 years within this data.**

But they are particularly efficient at determining the mortality at 5 years. The $R^2$ indicates that only 14.8% of the mortality at 5 years can be explained by the extent of the spread, the age group and "more than 4 nodes" factor.
</div>

```{r}
tab_model(  model_reg_all, model_reg_extent_age_node4, null_model, show.p=TRUE,  show.r2=TRUE, show.stat = TRUE, show.aic = TRUE, show.dev = TRUE)
```

## ANOVA  
<div class = "blue">
Here we use ANOVA to check if the chosen model deviance is significantly different from the deviance of the NULL model.  The null hypothesis is that the deviances are the same.

<p style="border:3px; border-style:solid; border-color:#2B547E; padding: 1em;">
$H_o: \mu_1 = \mu_2 = \mu_3 = ...$ risk of a Type 1 error = $\alpha = 0.05$ or something less.  
$H_a:$ at least one of the means is different from the others.
</p>

The below ANOVA test indicates that it is significantly different ( p = 0.00000000000000022 < 0.05)
</div>
```{r}
anova(null_model, model_reg_extent_age_node4, test ="Chisq")
```

## Predicting Against Testing Sample
Prior to using the above equation, we must ensure that it is suitable for our training sample first.

### Training model
Before we assess the correct equation based off of using the entire sample, it helps to look at the Null model, as well as the full model for the training sample.

#### The "NULL" Model Using the Training Sample

The NULL model for the training model is shown below:

```{r}
null_model_train <- glm(formula = mort_5yr ~ 1, 
    family = binomial(logit), data = train)
summary(null_model_train)
```

#### The Full Model Using Training Sample

Similar to the model generated from the full sample, the extent of the spread, the age group and the node4 appear to be the main predictors of the life status at 5 years.

```{r}
model_reg_all_train <- glm(mort_5yr ~ extent.factor + age.factor  + node4.factor + obstruct.factor + perfor.factor + adhere.factor + differ.factor + surg.factor + nodes + loccomp.factor + rx.factor + sex.factor + age, family= binomial(logit), data = train)
summary(model_reg_all_train)
```
Now we look at the equation's effectiveness within the training sample.  
```{r}
training_model <- glm(mort_5yr ~ extent.factor + age.factor + node4.factor, family= binomial(logit), data = train)
summary(training_model)
```
<div class = "blue">
**The selected predictors from the full sample appear to also be significant with the training model.**

The intercept represent the 'alive' odds of a patient with the following characteristics:  
  * The extent of local spread of the tumor is "Contiguous structures – meaning nearby organs"  
  * The age group is '<40 years'    
  * With less than 4 positive lymph nodes  

The odds being alive for this patient is $e^{-2.5156}$  = `r exp(-2.5156)`

The odds will increase to  $e^{-2.5156 + 3.2343}$ = `r exp(-2.5156 + 3.2343)` if the extent of the spread is submucosa., but will decrease further to $e^{-2.5156 -1.3925}$ = `r exp(-2.5156 +-1.3925)` if the patient has more than 4 positive lymphe nodes.
</div>
#### McFadden $R^2$
<div class = "blue">
The McFadden $R^2$ is often considered to be the most credible metric to indicate the approximate percentage of variance in life status that’s explained by the extent of the spread, the age group and the number of nymph nodes. However, its value of 11.95% is very modest.
</div> 
```{r}
M_r2 <- pR2(training_model)
M_r2
```

#### odds ratios and 95% Confidence Interval

```{r}
tab_model(
  training_model,
  title = "Logistic Regression Odds Ratios for life status at 5 years using the Training Sample",
  show.stat = TRUE,
  digits = 3,
  string.stat = "z-value",
  string.p = "p (sig)",
  show.fstat = TRUE,
  show.dev = TRUE,
  show.aic = TRUE,
  CSS = list(
    css.depvarhead = 'color: red;',
    css.centeralign = 'text-align: left;',
    css.firsttablecol = 'font-weight: bold;',
    css.summary = 'color: blue;'
  )
)
```


```{r}
pacman::p_load(ggplot2)
plot_model(
  training_model,
  title = "Logistic Regression of life status at 5 years using the Training Sample",
  show.p = TRUE
)
```

#### Using the Model

The model coefficients can be used to predict the 'alive' probability a person with a given tumor extent, of a particular age group and having "more than 4 positive lymph nodes" or not.

The first 6 people in the training sample are shown below.

```{r}
head(train[, c(2, 22, 26, 24, 30)]) %>% datatable(  rownames = F, filter="top", options = list(pageLength = 10, scrollX=T))
```
<div class = "blue">
Probability of being alive if you’re the person with RID = 2 above whose spread of the tumor is 'serosa', is over 60 years old and has less than 4 nymph nodes calculates as:
$e^{-2.5156 + 1.7759 + 1.2298}$ = `r exp(-2.5156 + 1.7759 + 1.2298)`,which is a probability of `r exp(-2.5156 + 1.7759 + 1.2298) / (1+exp(-2.5156 + 1.7759 + 1.2298))`

The probabilities of being alive for the first 6 subjects are below:

```{r}
training_model$ fitted.values[1:6]
```

These probabilities can be compared with the actual life status of each patient:
</div>
```{r}
probs <- training_model$ fitted.values[1:6]
mort_5yr_model <- ifelse(probs > 0.5, "Alive", "Died")
head(cbind(train[, c(2, 22, 26, 24, 30)],mort_5yr_model)) %>% datatable(  rownames = F, filter="top", options = list(pageLength = 10, scrollX=T))
```

## Predicting Using the Training Model on the Test Sample
### Hit Ratio
The Hit Ratio tests the accuracy of an equation to get the same results on the test sample, as it did on the training sample.  
<div class = "blue">
The Hit ratio is $\frac{65+27}{137}$ or 67.15%. This is not a good model as we would hope to reach 80% of accuracy.
</div>
```{r}
predicted.prob <- predict( training_model, newdata=test, type = "response")
predicted.classes <- as.factor( ifelse( predicted.prob > 0.5, "Alive", "Died" ) )
predicted.classes <- fct_relevel( (predicted.classes) , c("Died","Alive" ) )
library(forcats)
tab_xtab(test$mort_5yr, predicted.classes, var.labels = c("Actual Life status", "Predicted life status"), show.row.prc = TRUE)
```

### Receiver Operating Characteristic (ROC)
<div class = "blue">
This ROC representation basically helps to determine the quality of models. A model having an AUC (area under the curve) very close to 1 is the best and 1, or 100%, is perfect. Here it is 0.74 which is much better than guessing, but not a reliable prediction.
</div>
```{r}
library(pROC)
test_model <- glm(mort_5yr ~ extent.factor + age.factor + node4.factor, family= binomial(logit), data = test)
invisible(plot(roc(factor(ifelse(test$mort_5yr == "Died", 2, 1)), fitted(test_model)), print.thres = c(.1, .5), col = "red", print.auc = T))
```

### Checking The Residuals. 

```{r}
actual <- test$mort_5yr  # actual  
actual.num <- ifelse( test$mort_5yr == "Alive", 1, 0)
frq( actual.num , out="v", title="Actual mort_5yr as 0 and 1 for test sample" )
```


```{r}
residuals.num <- actual.num - predicted.prob
res.act.pred <- as.data.frame(cbind(actual.num, predicted.prob, residuals.num, test[c(22, 26, 24, 30)]) )
library(kableExtra)
head(res.act.pred, 20) %>%
  kable("html", align = 'clc', digits=5, col.names=c( "actual", "predicted", "residuals", "node4", "extent", "age group", "mort_5yr")) %>%
    kable_styling(bootstrap_options = "striped", full_width = F, position = "left") %>% 
    footnote(general = "The actual and predicted values of mort_5yr and the residuals.")
```
<div class = "blue">
The following plot shows the predicted probabilities on the horizontal axis and the residuals on the vertical axis. Since the predictions naturally gravitate towards 0 and 1 as the actual values, there are two lines in the plot. The important point is that the distance between the two lines stays fairly consistent from left to right.
</div>
```{r}
plot_scatter( res.act.pred, predicted.prob, residuals.num )
```

## Findings
<div class = "blue">
The extent of the spread, the age group and the number of nymph nodes are the main predictor of a patient life status at 5 years.  **The treatment is not a predictor.**

Even so, the model can not be used reliably to determine the 5 year life status of a patient; its hit-ratio doesn't exceed 67%.
</div>

# Multiple regression using all predictor variables.
<div class = "blue">
A stepwise regression is also use to analyze the impact that treatment and other factors have on life expectancy.  We use the same independent factors as the logistic regression.  Using stepwise (or multiple linear) regression requires that the dependent variable is numeric - which is why we are using 'time.years' instead of the factor mort-5yr.  This is dont to compare the results and add or remove validity from the logistic regression.  Moreover, it helps in determining the overall fit of the model and the relative contribution (or strength) of each of the independent variable in explaining the variance. 

The hypotheses for this is as follows:
<p style="border:3px; border-style:solid; border-color:#2B547E; padding: 1em;">
$H_o$: coefficient  = 0, a risk of a Type 1 error, = $\alpha = 0.05$  
i.e., no statistically significant relationship between each independent variable and the dependent variable.  
$H_a$: coefficient ≠ 0, a risk of a Type 1 error, = $\alpha = 0.05$  
.e., significant relationship between each independent variable and the dependent variable.
</p>
</div>
```{r ,  fig.width=5, fig.height=5, comment=NA}
RegModel.1 <- 
  lm(time.years ~ rx + sex + age + obstruct + perfor + adhere + differ + extent+surg+loccomp+node4,
   data=colon.data)
tab_model(
 RegModel.1 ,
  title = "Regression Model of time.years (based on full sample)",
  show.stat = TRUE,
  digits = 3,
  string.stat = "t-value",
  string.p = "p (sig)",
  show.fstat = TRUE,
  show.dev = TRUE,
  show.aic = TRUE,
  CSS = list(
    css.depvarhead = 'color: red;',
    css.centeralign = 'text-align: left;',
    css.firsttablecol = 'font-weight: bold;',
    css.summary = 'color: blue;'
  )
)
```
<div class = "blue">
In the table above, predictor variables 'extent'and 'node4' have coefficients that are significantly different (p<0.05) from zero as shown by the p-values (p(sig)). 
Other variables have coefficients that are not significantly different from zero indicating no significant relationship between each independent variable and the dependent variable.
</div>
<br>
This can also be displayed graphically.
<br>
<div class = "row">
<div class = "col-md-4">
<br>
<div class = "blue">
This graph shows those variables whose coefficients are significantly (p<0.05) different from zero in red and those not significantly different from zero in blue.

In this case, Variables 'rx', 'sex','obstruct', 'perfor', 'adhere','differ', 'surg',and 'loccomp' are shown in blue as they have coefficients that are not significantly different from zero (p>0.05).  

Variables shown in red include,'extent' and 'node4'- which have  coefficients that are significantly different from zero (p<0.05).  This indicates a statistically significant relationship between each independent variable and the dependent variable. 
</div>
</div>
<div class = "col-md-8">
```{r plotreg, message=FALSE, echo=TRUE, fig.width=4, fig.height=5}
plotreg(RegModel.1, custom.model.names ="Regression of time.years")
```
</div>
</div>

## Reducing the model to significant predictors
<div class = "blue">
In the current regression model (RegModel.1), the independent variables 'rx[T.Lev+5FU]', 'rx[T.Obs]', 'sex','obstruct', 'perfor', 'adhere','differ', 'surg',and 'loccomp' variables have p-values greater than 0.05.  Since these variables would not be able to predict changes in the values of 'time.years', the elimination of statistically insignificant variables will increase the precision of the model. 
</div>

## Stepwise Regression
<div class = "blue">
We have applied the statistical significance criterion automatically with an algorithmic procedure called stepAIC() from the 'MASS' package. The purpose of the statistical significance criterion is to find the best model that has a set of independent variables that significantly influence the dependent variable. The AIC at the top of each regression compares the quality of a set of statistical models to each other. The model which has lower AIC value is the best model.  AIC will choose the best model from a set, but it won’t say anything about absolute quality. Therefore, once the best model has been selected, a hypothesis test will be run to figure out the relationship between the variables in the model selected and the time in years until death ('time.years').
</div>
```{r}
library(MASS)  
step <- stepAIC(RegModel.1, direction="both")
step
```

## Testing the model from the 'stepAIC' function

We run another linear model with the results from the 'stepAIC' function in order to assess its fit.
```{r ,  fig.width=5, fig.height=5, comment=NA}
RegModel.2 <- 
  lm(time.years ~ rx + extent + loccomp + node4,
   data=colon.data)
tab_model(
  RegModel.1,RegModel.2 ,
  title = "Regression models with 6 predictor variables and with 11 predictor variables",
  show.stat = TRUE,
  digits = 3,
  string.stat = "t-value",
  string.p = "p (sig)",
  show.fstat = TRUE,
  show.dev = TRUE,
  show.aic = TRUE,
  linebreak = TRUE,
  CSS = list(
    css.depvarhead = 'color: darkorange;',
    css.centeralign = 'text-align: left;',
    css.firsttablecol = 'font-weight: bold;',
    css.summary = 'color: blue;'
  )
) 
```


___RegModel.2 model: The model reduced to only variables having coefficients with significant differences from zero___
<div class = "blue">
The RegModel.2 model above (model to the right), obtained through the stepwise regression (or stepwise selection) has the variables 'rx','extent','loccomp' and 'node4' have coefficients that are significantly different (p<0.05) from zero.  Hence, the null hypotheses, which assume no relationships between the variables and the dependent variable ('time.years'), for these variables would be rejected. 

Moreover, The adjusted R2(0.145), coefficient of multiple determination, is slightly higher in the original model.  The AIC is smaller at 2469.4 - showing that it has a better fit.

This will be considered our final model and we will continue on with testing.
</div>
## Relative Importance of Predictors (Drivers) in the final model
<div class = "blue">
The table below presents all of the relative importance metrics from ‘relaimpo’. 
The first block in the Table provides the 'lmg' metric for each variable. 
The second block gives the coefficients when 1 predictor is entered through to all 4 predictors. lmg metric assesses the relative importance of each of the predictors to the relationship (between the dependent variable and the predictor variables) both in terms of explanation and for prediction.
</div>
```{r}
library(relaimpo) 
imp.model.2<-calc.relimp(RegModel.2,type=c("lmg"),rela = TRUE)
imp.model.2
```
<div class = "blue">
The below table and graph show the variables which have the largest impact on 'time.years'
</div>
```{r}
rel.lmg<- calc.relimp(RegModel.2, type="lmg", rela=TRUE)
pm<- data.frame(rel.lmg$lmg, rel.lmg$lmg.rank)
pm<-pm[order(rel.lmg$lmg.rank),]
pm <-data.frame(Features=rownames(pm), pm, row.names=NULL)
colnames(pm) <- c("Features", "Importance", "Rankings")
is.num <- sapply(pm, is.numeric)
pm[is.num] <- lapply( pm[is.num], round, 3)
tab_df(pm)

theme_set(  theme_bw( ))
ggplot(pm, aes(x= reorder( Features,  Importance ), y=  Importance ) ) +
  geom_bar(stat="identity", fill="blue", colour="black") + 
  coord_flip() + 
  ggtitle("Key Drivers of 'time.years (best model)") +
  geom_text(aes(label=format(Importance, digits=2), size=0.5, hjust=1))+
  theme(legend.position="none") +
  theme(axis.text.y=element_text(face="bold", color="black",size=12))+
  theme(plot.title=element_text(
                                face="bold", color="black",size=18))+
  theme(axis.title.x=element_text(
                                  face="bold", color="black",size=14))+
  theme(axis.title.y=element_text(
                                 face="bold", color="black",size=14))+
  labs(y= "Relative impacts of Variables", x= "Variables")

```
<div class = "blue">
On investigating the relative importance of the variables in the final model, it can be concluded that  __node4': more than 4 positive lymph nodes'__, has the greatest impact on years until death. While 'extent'(Extent of local spread) is the second most important driver.
</div>

## Testing Multicollinearity Using Variance inflation factor (VIF)
<div class = "blue">
Multicollinearity in a multiple regression model indicates that collinear independent variables are related in some fashion, although the relationship may or may not be casual.  This reduces the precision of the estimate coefficients, which weakens the statistical power of the regression model. Hence, we may not be able to trust the p-values to identify independent variables that are statistically significant.  This is using the same method as was done in Section 5 for Logistic Regression.
</div>

```{r}
library(car)
(vf <- vif(RegModel.2) )
```
<div class = "blue">
In our final model (evident from the VIF table above), VIF values for all the predictor variables are well below the threshold level of 4. Hence, it may be concluded that multicollinearity does not affect the final model in predicting the outcome of the dependent variable.
</div>


## Building a Regression Model Using the Training Sample Data.

<div class = "blue">
We use the same training sample as in the Logistic Regression in hopes to acheive the same results.
</div>
```{r }
model.tr<-lm(time.years~ rx + extent + loccomp +  node4, train)
summary(model.tr) 
tab_model(
  model.tr,
  title = "Regression models of time.years using the Training Sample",
  show.stat = TRUE,
  digits = 3,
  string.stat = "t-value",
  string.p = "p (sig)",
  show.fstat = TRUE,
  show.dev = TRUE,
  show.aic = TRUE,
  CSS = list(
    css.depvarhead = 'color: red;',
    css.centeralign = 'text-align: left;',
    css.firsttablecol = 'font-weight: bold;',
    css.summary = 'color: blue;'
  )
)
```
<div class = "blue">
Within the training sample, it is noted that the same predictor variables are noted as significant as they were in the total sample.
</div>

## Reducing the model, using Stepwise Regression, to those variables having coefficients significantly different from zero

Stepwise regression was used to find the best training model with subset of predictor variables that are statistically significant.

```{r}
library(MASS)  
step <- stepAIC(model.tr, direction="both", trace = TRUE)
step
```
```{r include=FALSE}
model.tr<-step
```
<div class = "blue">
In case of current stepwise regression for time.years using the Training Sample, the model with variables 'extent', 'loccomp', and 'node4' have been chosen as the best training model. 

This is displayed graphically below using the plotreg() function.
</div>
```{r}
plotreg(model.tr, custom.model.names =" Regression models of time.years using the Training Sample")
```
## Reporting on the relative importance of those variables that remain in the model with a table, graph and in words. 
We again test the relative importance of our predictor variables to see their importance.  This is shown in a chart and a graph as well.
```{r}
library(relaimpo) 
imp.model.3<-calc.relimp(model.tr,type=c("lmg"),rela = TRUE)
imp.model.3
rel.lmg<- calc.relimp(model.tr, type="lmg", rela=TRUE)
pm<- data.frame(rel.lmg$lmg, rel.lmg$lmg.rank)
pm<-pm[order(rel.lmg$lmg.rank),]
pm <-data.frame(Features=rownames(pm), pm, row.names=NULL)
colnames(pm) <- c("Features", "Importance", "Rankings")
is.num <- sapply(pm, is.numeric)
pm[is.num] <- lapply( pm[is.num], round, 3)
tab_df(pm)
theme_set(  theme_bw( ))
ggplot(pm, aes(x= reorder( Features,  Importance ), y=  Importance ) ) +
  geom_bar(stat="identity", fill="blue", colour="black") + 
  coord_flip() + 
  ggtitle("Key Drivers of time.years in Training Model") +
  geom_text(aes(label=format(Importance, digits=2), size=0.5, hjust=1))+
  theme(legend.position="none") +
  theme(axis.text.y=element_text(face="bold", color="black",size=12))+
  theme(plot.title=element_text(
                                face="bold", color="black",size=18))+
  theme(axis.title.x=element_text(
                                  face="bold", color="black",size=14))+
  theme(axis.title.y=element_text(
                                 face="bold", color="black",size=14))+
  labs(y= "Relative impacts of Variables", x= "Variables")

```
<div class ="blue">
It is noted once again that'node4' has the highest lmg value, followed by the 'extent', "loccomp", then 'rx' when assessing their impact on 'time.years'. 
</div>
## Reporting The Training Residuals and Testing Their Distribution
Below is a table that shows the actual value of time.years for the sample, followed by the predicted value based on our model, then the residuals - or difference between them.
```{r}
predicted <- predict(model.tr, newdata=train  )
actual <- train$time.years # actual Q3a attitudes for testing sample
residual <- predicted - actual   
x<- as.data.frame(cbind(actual, predicted, residual)) 
library(kableExtra)
head(x, 10) %>%
  kable("html", align = 'clc', digits=2, col.names=c( "actual", "predicted", "residuals")) %>%
    kable_styling(bootstrap_options = "striped", full_width = F, position = "left") %>% 
    footnote(general = "The actual and predicted values of time.years and the residuals for the first 6 Patients.")
```
<div class ="blue">
In the table above, it can be noticed (from the first row) that the actual time.years'(years until death), for the first patient has the value of 8.46. The value of that patient’s years until death is predicted to be 5.18 by the final training model equation using the training data sample.

The difference also known as the residual or error of prediction is -3.28 meaning the equation's result was lower.  
</div>
## Testing if the distribution of the residuals is normal
<div class ="blue">
Prediction intervals are calculated based on the assumption that the residuals are normally distributed. Moreover, while building a optimally fitting linear model, it is assumed that the relationship is linear, and that the errors, or residuals, are simply random fluctuations around the true line. Non-normality of the residuals indicates that the errors the model makes are not consistent across variables and observations (i.e. the errors are not random). Hence model inference (confidence intervals, model predictions) may not be valid, affecting the reliability and usability of the model.

<p style="border:3px; border-style:solid; border-color:#2B547E; padding: 1em;">
$H_o$: Distribution of residuals is Normal , risk of a Type 1 error = $\alpha = 0.05$.   
$H_a:$ Distribution of residuals is not Normal
</p>
</div>
```{r  fig.width=5, fig.height=5 }
library("car")
qqPlot(model.tr, main="QQ Plot") 
```
<div class ="blue">
The plot above shows the distribution of the residuals against the expected normal distribution. For normally distributed data, observations should lie approximately on the reference line. In the Q-Q Plot above, the studentized Residuals match the theoretical quantiles only at the median. The samples seem to be symmetric around the median. Q–Q plot is "S" shaped, indicating that one of the distributions is more skewed than the other, or that one of the distributions has heavier tails than the other. Hence residuals may not be normally distributed. Hence, further investigation is required.

In the density plots below, we attempt to visualize the underlying probability distribution of the residuals by drawing an appropriate continuous curve. 
Residuals do not seem to be normally distributed as residuals are not symmetric about the mean and show bimodality. It should be further investigated by the conducting Normality testing.
</div>
```{r fig.height=5, fig.width=5, message=FALSE, warning=FALSE, comment=NA, include=FALSE}
d <- densityPlot(residual)
```
```{r echo=TRUE, message=FALSE, warning=FALSE, fig.width=5, fig.height=5, comment=NA}
plot(d, main=" Density Plot Residuals")
polygon(d, col="orange", border="green")
```

### Testing the hypothesis using Shapiro-Wilk’s method

Shapiro-Wilk’s method has been used to test the normality of Residuals, considering a significance level of 0.05. 

```{r echo=TRUE, message=FALSE, warning=FALSE, fig.width=5, fig.height=5, comment=NA}
shapiro.test((residual))
```
<div class ="blue">
From the Shapiro-Wilk normality test, we have got the p value of 0.0002281. Assuming an alpha of 0.05, since the p-value is greater than 0.05, null hypothesis (that distribution of residuals is Normal) is not rejected. Hence it may be concluded that the residuals are ___normally distributed___

Hence it can be concluded that the regression model (model.tr) can explain/ predict that the trends in the dependent variable(time.years )

## Testing homoscedasticity and Stating hypotheses. 

The assumption of homoscedasticity (meaning “same variance”) is central to linear regression models.  Homoscedasticity describes a situation in which the error term (that is, the “noise” or random disturbance in the relationship between the independent variables and the dependent variable) is the same across all values of the independent variables. 

Homoscedasticity in the best model  is tested below using the Breusch Pagan Test. It tests whether variances of residuals from a regression are dependent on the values of an independent variable.

__Stating Null and Alternative Hypothesis:__ 

$H_o:$ there are equal or constant variances,  $\alpha$ = 0.05

$H_a:$ there are unequal or non-constant variances    

```{r}
pacman::p_load( "olsrr" )
ols_test_breusch_pagan(model.tr)
```

The Breush-Pagan test creates a statistic that is chi-squared distributed and for this data that statistic = 4.73163.
The p-value is the result of the chi-squared test and the null hypothesis is rejected for p-value < 0.05. With the p value of 0.029 (<0.05), __the null hypothesis__ of homoskedasticity would be __rejected__.We should conclude that variance
around regression line is not constant throughout its range.

## Using the model based on the training sample to predict 'time.years'(years until death) for those in the testing sample. Assessing the quality of model based on the training sample when used on the testing sample data.  

## Predicting 'time.years' using the training model on the testing dataset

“predict()” evaluates the model equation on values of 'age', 'differ', 'extent', 'loccomp','node4' for each respondent in the testing sample. While the model was based on those respondents in the training sample, the respondents in the testing sample did not contribute to those calculations and can be considered to be “fresh” or “new” subjects.

__The code-chunk below predicts the time.years for the test, or holdout, sample of patients using the model built on the training data.__

```{r}
model.te<-lm(time.years ~ age + differ + extent + loccomp + node4, test)
predicted.tr.te <- predict(model.tr, newdata=test  )
predicted.tr.te
actual <- test$ time.years # actual attitudes for the testing sample
x<- as.data.frame(cbind(actual, predicted.tr.te)) 
```

### Assessing the model based on the training sample when used on the testing sample data. 

#### Scatter plots of Predicted vs Actual

```{r,  fig.width=5, fig.height=5}
scatterplot(x$predicted~x$actual, regLine=TRUE, smooth=FALSE, boxplots=FALSE, data=colon.data,col = "red", main = "Scatterplot of Predicted vs Actual " ,xlab = "Actual",ylab = "Predicted")
```

Scatter plots of Predicted vs Actual is used to understand how well the regression model makes predictions for different response values.  A perfect regression model has a predicted response equal to the true response, so all the points lie on a diagonal line. The vertical distance from the line to any point is the error of the prediction for that point. A good model has small errors, and so the predictions are scattered near the line.

Hence, it may be concluded that the model has a weak Goodness of fit since most of the points are foggy or dispersed (away from the diagonal line).

From this scatter plot, it can also be inferred that the residuals are heteroskedastic. This means that the variance of the error is not constant across various levels of the dependent variable. As a result, the standard errors of regression coefficients are unreliable and may be understated.

The analysis above, further mandates the assessment of goodness of the model based on the training sample when used on the testing sample data. 

## Correlation between actual and predicted values of the dependent variable time.years

___Stating the Hypothesis___

Ho: correlation = 0 i.e., actual and predicted values of the dependent variable, time.years, are __statistically independent__, a risk of a Type 1 error, = α=0.05

Ha: correlation ≠ 0 i.e., actual and predicted values of the dependent variable, time.years, are __statistically correlated__ , a risk of a Type 1 error, = α=0.05 are correlated a risk of a Type 1 error, = α=0.05

___Testing Hypothesis___

```{r,  fig.width=5, fig.height=5}
( c<- cor.test(x$actual,x$predicted))
library(sjPlot)
tab_corr(x, digits=2, show.p=TRUE, p.numeric=TRUE)
```
___Interpretations___

From the analysis above, it can be inferred that actual and predicted values of the dependent variable, Life Expectancy, are significantly correlated (r = 0.365843  , p = 0.00000005116). Since p < 0.05, the null hypotheses can be rejected. Also, the confidence interval does not include 0.
The correlation between the actual and predicted values for those in the test sample is very high, indicating that the predicted values that are close to the actual data values. Hence, Prediction has been done well. 

## Conclusion

The regression model is  very well able to predict the dependent variable,time.years'(years until death)

# Survival analysis

Just like with the regression models, we have to start with a null model.  The table for KM analysis on all cases for the null model is below.

```{r }
surv.colon <- survfit(Surv(time, status) ~ 1,  data = colon.data)
summary(surv.colon)
```

This can be displayed visually with the corresponding graph. 

```{r }
plot(surv.colon,  xlab="Days till death", ylab="Survival Probability")
title("Survival curves for colon data, Null model")
```

```{r }
library(survminer)
ggsurvplot(surv.colon, data = colon.data, pval = TRUE)
```
<div class = "blue">
Since RX treatment has not been entered into the model above, the table and graph above convey that the probabilities of survival from colon cancer at each point in time (days). The lower the days, the higher probability of survival. At 23 days time, there are 551 people at risk with a 99% chance of survival. However as the days increase, the chances of survival go down. We see this illustrated in both the graph and table. For example, at 797 days time, there are 417 people with a chance of survival below 76%.
</div>

## Survival Model for the RX Treatment on All Cases

First this is done by showing the model for each treatment type.
```{r }
surv.colon <- survfit(Surv(time,status) ~ rx, conf.int=TRUE, data = colon.data)
summary(surv.colon)
```
Then displayed graphically
```{r }
plot(surv.colon, lty = c(2,1), xlab="Days to next event", ylab="Survival Probability", col=c("red","green"))
legend("topright", legend=c("Probability of survival: Standard Treatment", "Probability of survival: Test Treatment"),fill=c("red","green"),bty="n")
title("Survival curves for Lung Cancer data")
```

```{r}
ggsurvplot(surv.colon, data = colon.data, pval = TRUE, conf.int = TRUE, risk.table = TRUE,  surv.median.line = "hv", ggtheme = theme_bw(),  palette = c("#E7B800", "#2E9FDF"), ncensor.plot = TRUE, break.time.by = 300)
```
<div class = "blue">
Above we see the survival analysis divided into two groups. Group one being the Lev treatment and group 2 being the Lev+5FU treatment. Some observations include:

For the Lev treatment group, in 1509 days time, 159 people had survived and were at risk with a 56% chance of survival. 

For the Lev+5FU treatment group, in 1511 days time, 176 people had survived and were at risk with a 65% chance of survival.

In the plot above:

* The horizontal axis (x-axis) represents time in days, and the vertical axis (y-axis) shows the probability of surviving or the proportion of people surviving. The lines represent survival curves of the two treatment groups. A vertical drop in the curves indicates an event. The vertical tick mark on the curves means that a patient was censored at this time.
* Confidence bands for each survival curves are displayed in their respective colors.
* Confidence bands are wide at the tail.
* At time zero, the survival probability is 1.0 (or 100% of the participants are alive).
* From the time of approx 10 to 100, probability of survival for those on standard treatment are higher than probability of those on test treatment.
* After the time approx 100, the probability of survival for those on the standard treatment gets lower compared to the probability of survival for those on the test treatment.
* From the graph, it shows the test treatment is less effective than standard treatment in the beginning phase (till time 100) and after that test treatment is superior than the standard treatment.
* p-value is the p-value of the Log-Rank test comparing the treatment groups
* Risk table below the plot shows absolute number of individuals at risk for both the treatment groups.
* n.censor plot displays the number of censor subjects at the time. There is no censor after time approx 250.
</div>

</div>
## Test statistically if the two treatments are the same or different.

Let's define our hypotheses
<p style="border:3px; border-style:solid; border-color:#2B547E; padding: 1em;">
$H_o:$ There's no difference between the Lev treatment and Lev+5FU treatment, $\alpha = 0.05$  
$H_a:$ There is a difference 
</p>

A statistical analysis can be done to prove the differences within the treatment.
```{r }
survdiff(Surv(time, status)~ rx, data=colon.data, rho=0)
```
<div class = "blue">
Above we see a chi square value of 4.1 and a p value of .04. We do reject the null hypothesis as the p-value is .04 and  below our threshold of .05. This means there is a  difference between the Lev treatment and Lev+5FU colon cancer patients.
</div>
# Summative Conclusion
## Distributions of Variables
## Correlation of Variables
## Stepwise (Linear) Regression
## Logistic Regression
## Survival Analysis
# Application
